---
layout: default
title: Attention Is All You Need
---
## **论文链接**
- http://arxiv.org/abs/1706.03762v7

## 摘要

本文提出了一种全新的序列到序列转导（sequence transduction）模型架构——Transformer，它完全基于注意力机制（Attention Mechanism），摒弃了以往主流的循环神经网络（RNN）和卷积神经网络（CNN）。Transformer不仅在英德和英法翻译任务上取得了超越以往所有模型的新纪录，还显著提升了模型并行性和训练效率。此外，Transformer在句法分析等任务中也表现良好，显示其广泛的泛化能力。

## 引言

序列建模和转导问题历来采用循环神经网络和其衍生架构（如长短时记忆网络LSTM、门控循环网络），这些模型在语言建模和机器翻译等任务中表现突出。然而，RNN固有的逐步计算过程限制了并行化能力，尤其在长序列时更加突出。虽有部分高效改进，但本质上的顺序依赖始终存在。引入注意力机制后，模型能够有效建模远距离依赖，但通常仍结合于RNN结构之中。本工作中，作者提出Transformer，完全基于注意力机制，通过全局依赖建模实现高效并行，并在翻译任务上取得领先性能。

## 背景

为减少顺序计算，近年来涌现了诸如Extended Neural GPU、ByteNet、ConvS2S等卷积网络架构，它们在输入和输出各位置并行计算隐藏表示。但这些架构在建模长距离依赖时，所需运算步骤随位置距离线性或对数增长，难以捕捉跨长距离的上下文。相比之下，Transformer通过自注意力机制（Self-Attention），在任意输入输出位置间仅需常数步操作，从而极大提升了效率与表现力。同时，自注意力机制已在阅读理解、摘要生成、文本蕴含等领域广泛应用，但Transformer首次将其作为序贯转换的唯一结构基础，摒弃了所有序列对齐的循环或卷积。

## 模型架构


Transformer采用经典的编码器-解码器（Encoder-Decoder）架构。编码器与解码器均由多层堆叠构成，每层包括多头自注意力机制（Multi-Head Self-Attention）和逐位置前馈神经网络（Feed-Forward Network），通过残差连接和层归一化确保训练稳定。

### 编码器与解码器堆栈

- **编码器**：由6层组成，每层包含多头自注意力子层和逐位置前馈网络，两者均采用残差连接及层归一化，所有输出维度一致。
- **解码器**：同样由6层组成，多出一层对编码器输出的多头注意力。此外，解码器自注意力通过mask防止未来信息泄漏，确保自回归输出。

### 分类体系核心：注意力机制的三大类型

Transformer的创新分类体系体现在其“注意力机制”的三重使用方式：

1. **编码器-解码器注意力（Encoder-Decoder Attention）**
   查询（Query）来自解码器当前层，键（Key）和值（Value）来自编码器输出，使解码器能全局关注输入。
   *对应传统Seq2Seq的注意力链接方式。*

2. **编码器自注意力（Encoder Self-Attention）**
   查询、键和值均来自编码器前一层，可全局聚合输入信息，建模序列内任意位置之间的关系。

3. **解码器自注意力（Decoder Self-Attention）**
   查询、键和值均来自解码器前一层，但通过mask确保每个位置只能访问当前及之前位置，维持自回归特性。


#### 标准维度说明

- **分头机制**：注意力不是单头而是多头（如$h=8$），每头独立学习低维子空间表示并并行处理，最终拼接并投影输出，提升模型对多方面特征的捕捉力。
- **投影参数**：每头有独立线性变换，保证不同子空间聚合多样信息。
- **自注意力与点积缩放**：通过$\frac{QK^T}{\sqrt{d_k}}$防止大维度下点积分布过于尖锐，使梯度更稳定。

### 其它结构模块

- **逐位置前馈网络**：每层每位置单独两层全连接，带ReLU激活。
- **嵌入与Softmax**：输入输出都通过相同权重矩阵进行嵌入，最后线性变换加Softmax输出概率。
- **位置编码（Positional Encoding）**：注入序列顺序信息，采用正弦-余弦周期函数（或可学习向量），使模型可识别顺序与位置信息。

### 分类体系总结

该架构的分类体系以“注意力流向”和“模块位置”为核心维度：

- **自注意力vs跨模块注意力**：同一序列（自注意力）或跨序列信息（编码器-解码器注意力）。
- **头数/投影方式维度**：单一注意力vs多头并行，影响表达力和泛化能力。
- **Masking类型维度**：决定模型能否适用于自回归、并行生成等场景。

## 为什么选用自注意力

通过对比分析，作者显示自注意力在以下三个方面优于循环与卷积结构：

1. **每层计算复杂度**：自注意力为$O(n^2 \cdot d)$，RNN为$O(n \cdot d^2)$，卷积为$O(k \cdot n \cdot d)$。
2. **可并行化性**：自注意力仅需常数步，可完全并行；RNN需$O(n)$步；卷积可并行，但序列间仍有限制。
3. **建模长距离依赖的最短路径**：自注意力为常数步（任意两点均可直连），RNN/卷积随距离线性/对数增长，难以捕捉远距离信息。

此外，多头机制提升了模型解释性，注意力分布可揭示模型聚焦的句法、语义结构。

## 训练细节

- **数据与Batch策略**：采用WMT大规模英德、英法语料，分组按近似长度批处理。
- **硬件与调度**：8块P100 GPU，基础模型训练12小时，增强版3.5天。
- **优化器**：Adam，动态调整学习率，前期线性升温（warmup），后期反平方根下降。
- **正则化**：残差dropout（0.1），标签平滑（0.1）等多重手段避免过拟合。

## 结果

### 机器翻译

Transformer在WMT 2014英德任务上取得28.4 BLEU，是迄今最高水平，且训练成本仅为传统CNN/RNN模型的几分之一。英法任务同样以41.8 BLEU创下单模型纪录。

### 架构变体实验

更大规模模型、更多注意力头、适当维度均能提升性能。单头机制显著下降（-0.9 BLEU），而过多头数也损失性能。位置编码采用正弦/余弦或学习型均无明确差异。

### 句法分析泛化能力

在英语结构分析任务上，Transformer在小数据情况下超越所有传统判别模型，与最新生成模型（RNNG）表现接近，在大数据半监督设置下成为最优算法之一，表明其强泛化能力。

## 结论

Transformer作为首个全注意力序列转换架构，彻底摒弃循环与卷积，通过多头自注意力和高度并行计算，实现了前所未有的效率和性能突破。在翻译及句法分析等任务中均取得领先表现。作者认为注意力类模型前景广阔，计划将Transformer扩展到图像、语音、视频等多模态领域，并探索局部限制型注意力以应对超大输入输出场景，以及提升并行生成能力。

## 未来方向与关键挑战

1. **多模态扩展**：将Transformer应用于图像、音频、视频等非文本序列。
2. **局部/限制注意力机制**：为处理超长序列，探索仅聚焦局部邻域的注意力变种。
3. **更高效的生成方式**：降低生成流程的顺序性，实现真正的并行生成或规划式输出。
4. **兼容泛化挑战**：面向更长的序列与低资源场景，进一步提升泛化与可伸缩性。
5. **模型解释性分析**：深挖不同注意力头捕捉的语法/语义机制，提升模型可解释性。
6. **高效优化与硬件适配**：在算力资源有限情况下继续优化模型效率与性能。

## 注意力可视化

- ![注意力长距离依赖示例 Figure 3]( /data/paper/source/1706.03762v7/images/sec8_fig1_global3_img1.png )
- ![代词指向注意 Figure 4-1]( /data/paper/source/1706.03762v7/images/sec8_fig2_global4_img1.png )
- ![代词指向注意 Figure 4-2]( /data/paper/source/1706.03762v7/images/sec8_fig2_global4_img2.png )
- ![不同头学习不同句子结构 Figure 5-1]( /data/paper/source/1706.03762v7/images/sec8_fig3_global5_img1.png )
- ![不同头学习不同句子结构 Figure 5-2]( /data/paper/source/1706.03762v7/images/sec8_fig3_global5_img2.png )
