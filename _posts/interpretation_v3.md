---
layout: default
title: Attention Is All You Need
---

# TL;DR
Transformer 是首个完全基于注意力机制 (attention mechanism) 的序列转导模型，摒弃了循环和卷积结构，在机器翻译等任务上，既显著提升了性能又极大加速了训练流程，推动了自然语言处理进入全新范式。

# 关键定义

**1. 注意力机制（Attention Mechanism）**
一种将 query（查询向量）、key（键向量）与 value（值向量）映射到输出的机制，其中输出是 value 的加权和，权重由 query 与每个 key 的相似度决定。
*本文提出了“缩放点积注意力”（Scaled Dot-Product Attention）与“多头注意力”（Multi-Head Attention）为核心变体。*

**2. 多头注意力机制（Multi-Head Attention）**
并行运行多个注意力层，每个层拥有独立的参数，将输入按不同方式投影后分别聚合，最后再拼接，以提升模型捕获复杂依赖的能力。

**3. 位置编码（Positional Encoding）**
为弥补Transformer无卷积/无循环导致的序列顺序感缺失，提出将正弦和余弦函数编码加到词嵌入（embedding）上，使模型能理解序列中各 token 的位置关系。

# 相关工作

当前序列建模（如语言建模、机器翻译）领域主流方法为循环神经网络（RNN）、长短时记忆网络（LSTM）以及门控循环单元（GRU），并普遍采用编码器-解码器架构 (Encoder-Decoder Architecture)。这些方法虽表现优异，但存在两个关键瓶颈：
1. **计算高度顺序化**：每个位置依赖于前一时刻状态，限制了并行计算和大规模训练效率。
2. **长距离依赖建模受限**：即使卷积模型（如ByteNet、ConvS2S）在并行性或依赖建模上有所改善，仍需多层堆叠才能跨越长距离，导致路径过长、学习困难。

注意力机制已经在多种任务中成为关键组件，但过去多与循环网络结合使用，鲜有完全纯粹、独立的自注意力建模。**Transformer**直接瞄准上述瓶颈，提出完全基于注意力的解码架构，实现全序列任意位置互联及高度可并行。

# 本文方法

## 总体架构

Transformer采用经典编码器-解码器结构，但**核心创新点是用堆叠的自注意力与前馈网络完全取代循环与卷积层，并融合多头机制与位置编码。**

![模型架构图](images/1706.03762v7/ModalNet-21.png)

### 1. 编码器与解码器（Encoder & Decoder）
- **编码器**：由6层组成，每层包含两部分：多头自注意力（Self-Attention）+前馈网络（Feed-Forward Network），全部层间都用残差连接与层归一化 (Layer Normalization)，输出全为512维。
- **解码器**：同样6层，除上述两部分外，增加一层用来对编码器的输出进行注意力；此外，解码器自注意力通过mask保证位置i的输出仅依赖于已知的i之前预测结果，实现自回归(autoregressive)生成。

### 2. 缩放点积注意力（Scaled Dot-Product Attention）
- 公式如下：
  $$
  \mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^{T}}{\sqrt{d_k}})V
  $$
- 通过对Query和Key点积后缩放、softmax归一化，获得权重，再加权Value，实现高效注意力聚合。缩放因子 $\sqrt{d_k}$ 防止梯度消失/爆炸。

### 3. 多头注意力（Multi-Head Attention）
- 并行八个注意力头（$h=8$），每个头分别独立学习不同子空间依赖，最后拼接结果：
  $$
  \mathrm{MultiHead}(Q,K,V) = \mathrm{Concat}(\mathrm{head}_1, ..., \mathrm{head}_h) W^O
  $$
- 有效提升模型同时着眼不同语义/结构维度的能力。

![缩放点积及多头注意力图](images/1706.03762v7/ModalNet-19.png)
![多头注意力结构图](images/1706.03762v7/ModalNet-20.png)

### 4. 位置编码（Positional Encoding）
- 采用正弦/余弦固定式编码，使模型具备捕获序列顺序能力。
  $$
  PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{model}})
  $$
  $$
  PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d_{model}})
  $$
- 可选学得型嵌入，实验证明与固定式差异很小。

### 5. 前馈网络（Feed-Forward Network）
- 每层均包含一个两层全连接网络，激活函数为ReLU，逐位置单独计算。

### 6. 正则化与优化
- 采用残差Dropout、标签平滑（Label Smoothing）、Adam优化器及新型学习率预热（Warmup）调度，实现高效训练与泛化性能。

## 方法本质创新与优点

- **极致并行化**：所有输入输出位置间可一次性并行计算，无序列依赖，极大加速训练。
- **最短路径依赖建模**：任两位置之间路径恒为常数阶（O(1)），相比卷积/RNN，极大简化长距离依赖学习。
- **结构解释性更强**：注意力权重分布易于可视化和语法/语义结构分析。
- **泛化性高**：模型本身不依赖前序任务特定结构，对多种序列任务均适用。

# 实验结论

## 1. 机器翻译（Machine Translation）
- 在WMT 2014英德翻译任务，**Transformer (big)** 模型 BLEU 达28.4，较最好同类模型（含Ensemble）提升2分以上。
- 在英法任务单模型 BLEU 达41.8，训练成本低于前SOTA模型的1/4。
- 即使基础模型（base）也优于所有先前单模型/集成模型，充分证明架构优势。

## 2. 变体对比实验（Model Variations）
- 增加头数、多样化维度、增大模型规模均有助于提升BLEU，单头注意力效果明显较差。
- 用学得型位置嵌入代替正弦编码，性能无明显差距，为实际部署带来灵活性选择。
- Dropout等正则化措施有效防止过拟合，提升泛化性能。

## 3. 泛化任务——英语句法分析（English Constituency Parsing）
- 在仅使用40K WSJ训练的情况下，Transformer已能达到F1=91.3，与传统最优解析器持平甚至略有超越。
- 在半监督扩充下，F1可达92.7，接近该领域最高水平，证明模型结构的高度可迁移性。

## 4. 可解释性示范
- 多头注意力可聚焦于句法结构、长距离依赖与指代问题，分头学习不同语义关系。

![注意力可视化示例1](images/1706.03762v7/x1.png)
![注意力可视化示例2](images/1706.03762v7/x2.png)
![注意力可视化示例3](images/1706.03762v7/x3.png)
![注意力可视化示例4](images/1706.03762v7/x4.png)
![注意力可视化示例5](images/1706.03762v7/x5.png)

## 总结结论
Transformer模型通过完全抛弃循环和卷积，创新性地发挥多头自注意力与位置编码的威力，实现对序列数据的高效建模。理论与实验证明其在机器翻译等语言任务上极大提升了表现，并兼具训练效率与结果解释性，为自然语言处理智能体（Agent）领域开辟了全新路径。其高度并行性和广泛适用性预示着注意力机制将在多模态、长序列等更广泛智能体应用中不断取得突破。