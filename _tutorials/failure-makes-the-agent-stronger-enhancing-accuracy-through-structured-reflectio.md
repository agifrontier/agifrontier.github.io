---
layout: default
title: "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions"
---

# Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions

- **ArXiv URL**: http://arxiv.org/abs/2509.18847v2

- **作者**: Hengyu Shi; Tianyang Han; Junfeng Luo; Junhao Su; Junwei Yang

- **发布机构**: Meituan

---

# TL;DR
本文提出一种名为结构化反思 (structured reflection) 的新机制，将工具调用失败后的错误诊断与修复过程，转化为一种可控、可训练的智能体行为，从而显著增强大型语言模型在多轮工具交互中的可靠性和错误恢复能力。

# 关键定义
本文的核心是**结构化反思 (Structured Reflection)**。它是一种显式的、可训练的行为策略，旨在使智能体能够从工具调用的失败中学习和恢复。其核心思想是，当一次工具调用失败后，模型不应盲目重试，而应执行一个两步过程：
1.  **诊断 (Diagnose)**：智能体首先生成一段反思文本（例如，包裹在 $$<reflect>$$ 标签内），根据上一步的错误反馈和上下文证据，清晰地诊断失败的根本原因。
2.  **修复 (Repair)**：基于诊断结果，智能体提出一个修正后的、可执行的工具调用请求。

通过将“从错误到修复”的过程变成一个标准化的动作，并围绕它设计训练数据和奖励函数，该能力不再依赖于启发式提示，而是成为模型内在的、可靠的技能。

# 相关工作
目前，增强大型语言模型（LLM）工具使用能力的方法主要有监督微调（SFT）和强化学习（RL），这些方法主要优化单轮的、一次性的工具调用。这导致了以下关键瓶颈：
1.  **在多轮交互中表现脆弱**：一旦某次工具调用失败，模型往往会重复同样的错误，缺乏有效的错误恢复机制。
2.  **依赖单向推理**：现有的自我修正方法大多依赖于启发式提示（例如，让模型“多想一想”）或单向的思维链，模型难以准确定位并从根本上纠正错误。
3.  **奖励信号稀疏**：在工具调用任务中，参数、格式或工具名称的微小错误都可能导致整个调用无效，使得模型难以从失败中获得有效的学习信号。

本文旨在解决上述问题，特别是如何让智能体在多轮工具交互中，从失败中可靠地恢复，即将自我修正能力从一种不稳定的启发式技巧，转变为一种稳健、可学习的核心能力。

# 本文方法

本文的核心方法包含三个部分：一个为训练反思能力而构建的基准数据集 Tool-Reflection-Bench，一套为工具调用场景量身定制的精细化奖励机制，以及一个结合了 DAPO 和 GSPO 优点的强化学习目标函数。

### Tool-Reflection-Bench
为了让模型学会“从错误中反思和修复”，本文构建了一个名为 Tool-Reflection-Bench 的基准数据集。其构建过程分为三步：
1.  **基于扰动的破坏 (Perturbation-based Disruptions)**：首先，从未经污染的、正确的多轮工具调用轨迹 $D^{+}$ 出发。然后，定义一系列扰动算子 $P\_j$ 来模拟常见的失败模式，例如用错误的工具替换、重复调用、损坏参数等。将这些算子应用于某个正确的调用 $m^{\mathrm{ast}}\_{2k}$，生成一个错误的调用 $\tilde{m}^{\mathrm{ast}}\_{2k}$。

2.  **正样本转换 (Positive Samples Transformations)**：通过一个辅助 LLM 模拟外部工具对错误调用 $\tilde{m}^{\mathrm{ast}}\_{2k}$ 的报错信息 $\tilde{m}^{\mathrm{tool}}\_{2k+1}$。这样就构造出了一段包含失败证据的错误轨迹前缀 $D^{-}$。


{% raw %}$$
D^{-}=\mathrm{Perturb}\!\left(D^{+},P_{j}\right)=\Big(m^{\mathrm{sys}}_{0},\,m^{\mathrm{usr}}_{1},\,\ldots,\,\tilde{m}^{\mathrm{ast}}_{2k},\,\tilde{m}^{\mathrm{tool}}_{2k+1}\Big)
$${% endraw %}



3.  **反思修复过程 (Reflection Repair Process)**：将原始的正确调用 $m^{\mathrm{ast}}\_{2k}$ 和错误的调用 $\tilde{m}^{\mathrm{ast}}\_{2k}$ 及其结果一并呈现给 LLM，让其生成反思文本 $r$ 和修复后的调用 $c$。再经过人工监督和后编辑，得到高质量的反思 $r^{\star}$ 和确保正确的调用 $c^{\star}$。最终，一个完整的训练样本 $x$ 由此构成：


{% raw %}$$
x\;=\;\big(D^{-},\,r^{\star},\,c^{\star},\,D^{+}\_{>2k+1}\big)
$${% endraw %}


它包含了错误轨迹、标准的反思、正确的修复调用，以及原始轨迹的后续部分。训练集包含完整的“错误→反思→修复”链条，而测试集只包含错误轨迹，用于评估模型的自我修正能力。

### 奖赏设计
为了在强化学习中提供有效指导，本文设计了一套精细化的奖励机制，以应对工具调用中奖励稀疏的问题。
1.  **奖励分解**：将模型的生成内容 $C$ 和标准答案 $G$ 都分解为三个部分：反思文本 ($c\_{\text{ref}}$)，工具调用集 ($C\_{\text{calls}}$)，和最终回答 ($c\_{\text{final}}$)。分别计算这三部分的分数：反思和最终回答的得分 $s\_{\text{ref}}, s\_{\text{final}}$ 来自语义相似度，而工具调用的得分 $s\_{\text{call}}$ 则要求工具名称和参数完全匹配，是一个二元（0或1）分数。

2.  **动态归一化**：为了处理某些样本只包含部分组件（如只有工具调用，没有最终回答）的情况，奖励函数通过一个存在掩码 $(I\_{\text{r}}, I\_{\text{c}}, I\_{\text{f}})$ 和预设权重 $(w\_{\text{r}}, w\_{\text{c}}, w\_{\text{f}})$ 对总分进行归一化，确保最终得分 $S$ 始终在 $[0, 1]$ 区间内，避免因组件缺失导致分数被人为拉低。


{% raw %}$$
S\;=\;\dfrac{w\_{\text{r}}I\_{\text{r}}\,s\_{\text{ref}}\;+\;w\_{\text{c}}I\_{\text{c}}\,s\_{\text{call}}\;+\;w\_{\text{f}}I\_{\text{f}}\,s\_{\text{final}}}{W\_{\text{act}}}
$${% endraw %}



3.  **格式惩罚因子**：设计了专门针对格式错误的惩罚项，包括：
    *   $P\_{\text{miss}}$：惩罚模型未能生成标准答案中存在的组件。
    *   $P\_{\text{extra}}$：惩罚模型生成了标准答案中没有的额外组件。
    *   $P\_{\text{count}}$：惩罚模型生成的工具调用数量与标准答案不符。
    这些惩罚项被整合进一个格式因子 $F$，与内容得分 $S$ 相乘，得到核心奖励 $R\_{\text{core}} = S \cdot F$。

4.  **奖励回退机制 (Backoff)**：由于工具调用得分 $s\_{\text{call}}$ 是严格的完全匹配，这会导致奖励信号非常稀疏。为了在训练初期提供更密集的梯度信号，本文引入了一个回退机制：当核心奖励 $R\_{\text{core}}$ 低于某个阈值 $\varepsilon$ 时，系统会退而求其次，使用模型生成全文和标准答案全文的整体语义相似度作为奖励。这确保了即使模型未能完美匹配工具调用，也能根据其输出的整体质量获得一定的学习信号。


{% raw %}$$
R\_{\text{total}}\;=\;\begin{cases}\mathrm{clip}\_{[0,1]}(R\_{\text{core}}),&R\_{\text{core}}\geq\varepsilon,\\[4.0pt] \mathrm{clip}\_{[0,1]}\!\left(w\_{\text{b}}\cdot\mathrm{Sim}\!\big(\mathrm{concat}(C),\,\mathrm{concat}(G)\big)\right),&\text{otherwise,}\end{cases}
$${% endraw %}



### 用于Tool-Reflection-Bench的强化学习
本文采用了一种结合了 DAPO 和 GSPO 思想的强化学习目标函数，以稳定并高效地优化模型策略。
*   **融合DAPO思想**：
    1.  **动态提示组过滤**：在训练过程中，动态过滤掉那些学习信号微弱的批次（例如，所有采样结果都完全正确或完全错误），集中算力于有价值的样本。
    2.  **非对称裁剪**：使用非对称的裁剪范围 $(\varepsilon\_{\text{low}}, \varepsilon\_{\text{high}})$，允许在优势为正时有更大的更新幅度，鼓励模型探索更好的策略。
*   **融合GSPO思想**：
    1.  **序列级重要性采样**：在序列（整个生成文本）级别计算重要性采样比率 $r\_i(\theta)$，并在此粒度上进行裁剪。这解决了传统PPO中token级重要性比率与序列级奖励不匹配的问题，使训练过程更加稳定。


{% raw %}$$
r\_{i}(\theta)=\left(\prod\_{t=1}^{ \mid o\_{i} \mid }\frac{\pi\_{\theta}\!\left(o\_{i,t}\mid q,\,o\_{i,<t}\right)}{\pi\_{\theta\_{\text{old}}}\!\left(o\_{i,t}\mid q,\,o\_{i,<t}\right)}\right)^{\!1/ \mid o\_{i} \mid }
$${% endraw %}



通过这个混合目标函数，模型能够在奖励信号复杂且稀疏的工具调用场景下进行更稳定和高效的学习。

# 实验结论
本文在自建的 Tool-Reflection-Bench 和公开基准 BFCL v3 上进行了实验，验证了所提方法的有效性。
*   **核心优势验证**：实验结果表明，经过结构化反思训练的模型，在多轮工具调用的成功率和错误恢复能力上取得了显著提升。当面对失败的工具调用时，模型能够更准确地诊断问题并生成正确的修复调用。
*   **性能表现**：在 BFCL v3 基准测试中，本文方法不仅在多轮交互中表现出色，还减少了不必要的冗余调用，同时保持了与基线模型相当的单轮工具调用性能。这意味着该方法在增强鲁棒性的同时，没有牺牲基础的调用能力。
*   **最终结论**：将反思过程明确地建模为一个可学习的动作，并将其作为优化目标，能有效增强智能体在工具交互中的可靠性。这为训练能够从失败中汲取教训、不断变强的智能体提供了一条可复现的路径。