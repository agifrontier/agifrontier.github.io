---
layout: default
title: "A Systematic Study of Model Merging Techniques in Large Language Models"
---

# 大模型融合实测：6种先进方法惨败，竟不敌最简单的“加减法”？

<img src="/images/2511.21437v1/A__title.jpg" alt="" style="width:85%; max-width:600px; margin:auto; display:block;">

想不想把多个“偏科”的AI模型融合成一个全能的“六边形战士”，还不用花钱重新训练？

> ArXiv URL：http://arxiv.org/abs/2511.21437v1

模型融合（Model Merging）技术就承诺了这样一个美好的未来。它旨在将多个针对特定任务微调过的模型检查点（checkpoints）合并成一个单一的、更强大的模型。

然而，一篇来自亥姆霍兹慕尼黑中心等机构的最新研究却泼了一盆冷水：在大型语言模型（LLM）上，那些听起来高大上的先进融合算法，效果竟然一塌糊涂！

研究发现，最古老、最简单的方法反而成为了唯一的赢家。这究竟是怎么回事？

### 一场系统性的大比拼

为了搞清楚哪种模型融合方法对当今的LLM最有效，研究者们进行了一场大规模的系统性评测。

他们选取了6种主流的模型融合方法，在4个开源LLM（涵盖Llama 3和Qwen3系列）上进行实验。每个基础模型都有12个不同的微调版本，并在16个标准的LLM评测基准上进行评估。

整个评测流程如下图所示，堪称“地毯式”搜索：

<img src="/images/2511.21437v1/x1.jpg" alt="评测流程图" style="width:90%; max-width:700px; margin:auto; display:block;">

这6种方法可以分为两大类：

1.  **基于权重插值的方法**：直接对模型的权重参数进行数学运算。

2.  **基于子空间的方法**：在更抽象的低维“任务子空间”中进行操作。

接下来，让我们看看战况如何。

### “返璞归真”的胜利：任务算术（Task Arithmetic）

首先登场的是基于权重插值的三种方法，其中最引人注目的是**任务算术**（**Task Arithmetic, TA**）。

这个方法简单到令人发指：

1.  计算出每个微调模型相比于原始基础模型的“变化量”，即“任务向量” $ \Delta W\_{i} $。

    


    {% raw %}$$ \Delta W_{i}=W_{i}-W_{0} $${% endraw %}



2.  将这些“任务向量”加权求和，再加回到原始基础模型上。

    


    {% raw %}$$ W_{\text{merged}}=W_{0}+\lambda\sum_{i=1}^{n}\alpha_{i}\Delta W_{i} $${% endraw %}



简单来说，就是做了一系列“加减法”。

与TA一同比较的还有TIES-Merging和Model Stock，它们在TA的基础上引入了更复杂的机制，试图减少模型间能力的“冲突”。

然而，结果却出人意料。

<img src="/images/2511.21437v1/x3.jpg" alt="Task Arithmetic性能表现" style="width:90%; max-width:700px; margin:auto; display:block;">

如上图所示，横轴是融合模型的数量，纵轴是相对基础模型的平均准确率提升。

**任务算术（TA）是唯一能稳定实现“建设性干涉”（Constructive Interference）的方法**。这意味着融合后的模型不仅超越了基础模型，甚至常常比参与融合的任何单个模型都要强。

而TIES-Merging等方法，在融合多个模型后性能反而急剧下降。

### “高精尖”方法的滑铁卢：子空间融合

那么，那些理论上更先进的子空间方法表现如何呢？

这类方法，如TSV-Merge、Iso-C和Subspace Boosting，不再直接操作完整的模型权重。它们认为微调引入的“任务知识”存在于一个低维子空间中。通过对这个子空间进行变换（如正交化、缩放），可以更优雅地合并能力。

听起来是不是非常高明？但现实是残酷的。

<img src="/images/2511.21437v1/x6.jpg" alt="子空间融合方法性能表现" style="width:90%; max-width:700px; margin:auto; display:block;">

从上图可以清晰地看到，这三种子空间方法在LLM上几乎全军覆没。随着融合模型数量的增加，它们的性能都出现了显著的下滑，远不如基础模型。

这场“屠杀”表明，之前在视觉模型或小型语言模型上奏效的先进技术，并不能直接照搬到现代LLM上。

### 为什么“先进”反而失效？

为什么最简单的方法效果最好，而复杂的算法却惨遭失败？

研究者认为，关键在于**LLM微调的复杂性**。

子空间方法通常有一个隐含假设：不同任务是相对独立的，它们的“任务向量”在几何上是正交或近似正交的。这样，在子空间里进行操作才不至于互相干扰。

然而，在实际应用中，我们拿到的各种LLM微调模型，其学习到的任务可能是高度重叠和纠缠的。例如，一个代码微调模型和一个数学微调模型，它们的能力边界可能非常模糊。

在这种“混乱”的场景下，强行进行子空间变换，反而会破坏模型内部精细的知识结构，导致灾难性的性能衰退。

相比之下，简单的任务算术（TA）虽然朴素，但它保留了权重空间中的线性结构，恰好与LLM微调后“线性模式连接”（Linearly Mode-Connected）的特性相符，因此风险更小，效果也更稳健。

### 结论与启示

这项大规模研究给整个AI社区带来了几个清晰的启示：

1.  **简单即是美**：在LLM模型融合领域，目前最可靠、最有效的方法就是**任务算术**（**Task Arithmetic**）。如果你想合并几个微调模型，从这个简单的“加减法”开始准没错。

2.  **别迷信“先进”算法**：许多在其他领域被验证有效的复杂融合算法，在LLM上可能会水土不服。在应用前，必须进行严格的实证检验。

3.  **未来方向**：我们需要为LLM量身定制新的融合算法，而不是简单地移植旧方法。同时，探索“感知融合的微调”（Merging-Aware Fine-tuning）策略，即在微调时就有意让模型学习到互补的能力，可能会是放大模型融合效果的关键。

总而言之，模型融合的潜力依然巨大，但通往“六边形战士”的道路，或许需要我们回归本源，从最简单的地方重新出发。