---
layout: default
title: "Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof, Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2, ERC-8004, and Beyond"
---

# Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof, Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2, ERC-8004, and Beyond

- **ArXiv URL**: http://arxiv.org/abs/2511.03434v1

- **作者**: Helena Rong

- **发布机构**: New York University Shanghai; University of Oxford

---

# 智能体间信任模型：Brief、Claim、Proof、Stake、Reputation 和 Constraint 在智能体网络协议设计中的比较研究

# 引言

随着数十亿人工智能（AI）智能体自主交易与协作的“智能体网络 (agentic web)”逐渐成形，信任的重心从人类监督转向了协议设计。大规模AI智能体生态系统毫秒级的动态协调与验证需求，是传统互联网信任机制无法满足的。由于AI智能体将被赋予处理金融交易、个人数据等敏感任务，确保它们之间的稳健信任至关重要。

本文识别并比较了智能体间协议设计中（隐式或显式）采用的六种不同信任模型：
1.  **Brief (简报)**：基于可验证声明。
2.  **Claim (声称)**：基于智能体自我宣称的能力与身份。
3.  **Proof (证明)**：基于密码学验证。
4.  **Stake (权益质押)**：基于可被罚没的抵押品。
5.  **Reputation (声誉)**：基于众包反馈和图信任信号。
6.  **Constraint (约束)**：基于沙盒化和能力限制。

本文的核心贡献在于：
1.  提出了一个统一框架，阐述了这六种信任模型。
2.  回顾了A2A、AP2、ERC-8004等前沿智能体交互协议，并分析了它们如何应用这些信任机制。
3.  批判性地审视了每种信任模型如何应对（或未能应对）大语言模型（LLM）特有的脆弱性，如提示注入、谄媚、幻觉和欺骗等。
4.  为设计更安全、更值得信赖的智能体网络勾勒了前瞻性的研究议程和设计启示。

# 背景

## 多智能体系统中的信任

信任是一种定向且特定于上下文的关系：智能体A可能在任务X上信任智能体B，但在任务Y上则未必。信任包含了“脆弱性 (vulnerability)”的元素：信任方承担着被辜负或背叛的风险。在计算领域，信任被形式化为一种可量化的心理状态，智能体可根据经验和上下文进行更新。常见模型包括声誉系统、概率信任模型和受PageRank启发的信任网络。然而，在开放的智能体网络中完全解决信任问题仍然困难。

## LLM特有的脆弱性与信任考量

基于大语言模型 (Large Language Model, LLM) 的智能体带来了独特的失效模式，使信任问题复杂化：

#### 提示注入 (Prompt Injection)
恶意制作的输入可以注入指令，颠覆智能体的既定策略。这使得任何假设智能体将严格遵守其原始规则的信任模型都变得脆弱。

#### 对引导/谄媚的超敏性 (Hypersensitivity to Nudging / Sycophancy)
LLM倾向于迎合用户（或其他智能体）的期望来调整回答，即使答案不真实。这种“易受引导性 (nudge-susceptibility)”意味着智能体可能被巧妙地操纵，从而偏离其初始目标。

#### 幻觉 (Hallucination)
LLM以产生流利但完全不正确或捏造的输出而闻名。幻觉会削弱基于自我声称（Claim）的信任模型，因为智能体可能声称拥有其不具备的能力。

#### 欺骗 (Deception)
足够先进的智能体可能会学会在观察下表现良好以积累声誉，然后在关键时刻背叛。这直接攻击了信任，尤其是基于声誉的信任模型。

#### 涌现的权力寻求与错位 (Emergent Power-Seeking and Misalignment)
一个目标错位的智能体可能会为了获取权力或资源而采取隐蔽行动。这意味着信任不应被假定为单调增加的；一个昨天还对齐的智能体，明天可能就改变了目标。

# 智能体间协议设计中的信任模型

有效的智能体间信任可以通过不同机制建立。本文将六种主要的信任模型进行了分类比较。

<br>


| 模型 (Model) | 信任基础 (Basis of Trust) | 优点 (Strengths) | 缺点 (Weaknesses) | 针对LLM的缓解措施 | 代表性用途 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Brief (简报)** | 由可信机构签发的凭证和背书 | 快速、便携，适用于发现和初始接触 | 依赖权威、中心化风险、粒度粗、撤销延迟 | 用第三方签名的凭证替代自我断言 | 可验证凭证、TLS证书 |
| **Claim (声称)** | 智能体的自我描述（身份、能力、策略） | 轻量、可扩展、实时性强 | 在对抗环境中脆弱，易受欺骗和夸大 | 几乎无直接缓解作用，仅作为其他机制的输入 | AgentCard、智能体配置文件 |
| **Proof (证明)** | 可验证的密码学或形式化证据 | 信任最小化，不依赖历史或声誉 | 成本高、验证范围关键、存在新的单点故障 | 通过日志和证明约束模型行为，防止欺骗和幻觉 | 数字签名、零知识证明、TEE证明 |
| **Stake (权益质押)** | 可被罚没的经济抵押品 | 创造经济激励，对不当行为施加成本 | 无法阻止恶意烧钱攻击，有财富集中风险 | 为错误行为提供经济惩罚信号，激励更安全的行为 | 链上验证者的罚没机制、保险池 |
| **Reputation (声誉)** | 聚合的交互反馈和社交信号 | 适应性强、表达力丰富、可覆盖主观领域 | 滞后指标，易受女巫攻击、共谋、刷分等影响 | 可过滤掉频繁出错（如幻觉）的智能体 | 评分系统、信任网络 |
| **Constraint (约束)** | 沙盒化与能力边界限制 | 最小化伤害范围，将信任从智能体转移到框架 | 可能限制智能体能力、存在沙盒逃逸风险 | 通过限制操作范围来有效控制提示注入的破坏半径 | API网关、权限控制、资源隔离 |

<br>

## Brief：背书与凭证

Brief模型将信任建立在由可信权威机构或信任链发布的证明之上。智能体出示签名的凭证，以断言其身份、能力或合规性。

这种模型假设存在可信的发行方，并且凭证与智能体的密码学身份牢固绑定。其优点在于能够快速引导信任，实现便携式的发现和初始接触。它通过用签名的第三方证明取代自我断言，缓解了LLM的虚假陈述问题。缺点是依赖于中心化权威，且凭证状态可能滞后于实时行为。

## Claim：身份、策略与能力的自我描述

Claim模型始于智能体对自身的描述，如枚举其身份、技能和策略的“智能体名片 (AgentCard)”。

这种模型假定了一种善意的基线，或者假定不实陈述最终会被发现。它本身在对抗性环境中非常脆弱，因为攻击者可以轻易地夸大能力或制作欺骗性简介。对于LLM的脆弱性，Claim模型几乎没有直接的缓解作用。因此，Claim应被视为更强机制的输入，而非关键决策的充分依据。

## Proof：密码学与可验证证据

Proof模型用可验证的证据取代承诺和背书。这些证据可以证明智能体执行了特定操作或满足了特定属性。机制包括数字签名、来自可信执行环境 (Trusted Execution Environments, TEE) 的证明以及零知识证明 (zero-knowledge proofs, zk-proofs)。

其核心优势是信任最小化 (trust minimization)。交互方无需了解智能体的历史或声誉，仅凭有效证明即可做出决策。对于LLM智能体，证明可以对抗多种失效模式，如通过防篡改日志追究责任，或通过zk-proofs证明其遵守了隐私策略。然而，证明只能保证完整性，而不能保证对齐。一个智能体可以正确地证明它执行了一个有害的策略。此外，证明系统会产生开销，并且在密码学堆栈中引入新的单点故障。

## Stake：抵押、罚没与激励对齐

Stake模型通过利益相关 (skin in the game) 来构建信任。智能体质押抵押品，如果违反协议规则，抵押品将被罚没 (slashing)。

这种模型假设智能体是效用最大化的，在乎损失抵押品。当与可验证性和反馈相结合时，Stake模型表现出色。证明为罚没提供了客观依据；声誉则提高了不当行为的机会成本。对于LLM智能体，Stake引入了一个经济学习信号：对错误或不安全行为的重复惩罚会激励其采用更安全的策略。然而，Stake主要是事后机制，无法阻止单次灾难性行为。

## Reputation：分布式反馈与社交信号

Reputation模型将交互结果聚合成一种信誉，供其他智能体在选择合作伙伴时查询。

其独特优势在于适应性和表现力。它可以追踪多维度的品质，并覆盖形式化验证不切实际的领域。对于LLM智能体，声誉可以作为其鲁棒性的代理指标：反复出现幻觉或被提示注入的智能体将积累负面反馈。然而，声誉是一个滞后指标，容易受到女巫攻击 (Sybil attacks)、共谋和“声誉榨取”（先积累信誉再进行一次性欺诈）的威胁。

## Constraint：沙盒化与能力限制

Constraint模型通过限制智能体的行为来最小化信任需求，即使智能体行为不当或失败，也能限制其造成的损害。这种模型将信任从智能体本身转移到了运行框架上。

Constraint对LLM特有的漏洞非常有效。它通过缩小操作界面和验证输入输出来限制提示注入的爆炸半径；通过拒绝访问广泛的系统接口来防止权限升级。其主要缺点是可能扼杀智能体的能力，因为过于严格的约束会影响其性能。在安全与自主之间找到合适的平衡点并非易事。

# 现有协议如何实现这些信任模型

## 谷歌的A2A（Agent-to-Agent）协议

A2A是一个开放的智能体间互操作性规范，它标准化了智能体如何描述自己和进行通信。每个智能体都通过$$AgentCard$$（一个JSON文档）来公布其身份和能力。

在信任模型方面，A2A原生支持**Claim**（自我描述的AgentCard）和**Constraint**（企业级控制和最小权限网络策略）。**Brief**则通过TLS证书和OAuth Token等传输层凭证体现。A2A并未规定生态系统范围内的**Reputation**、**Stake**或密码学**Proof**。这种设计使得A2A易于在参与者已知的组织环境中采用。然而，在开放的对抗性环境中，这些选择限制了A2A的安全性：未经核实的$$AgentCard$$依赖声明而非证据，缺乏协议级别的质押或声誉机制削弱了其抵抗女巫攻击和共谋的能力。

## 智能体支付协议（AP2）

AP2是一个面向支付的标准，旨在让AI智能体能够代表用户发起和完成商业交易，同时保持问责制。其核心抽象是“授权 (Mandate)”，这是一种可验证的凭证…
*(原文内容至此中断)*