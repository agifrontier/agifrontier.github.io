---
layout: default
title: "Multi-Agent Evolve: LLM Self-Improve through Co-evolution"
---

# Multi-Agent Evolve: LLM Self-Improve through Co-evolution

- **ArXiv URL**: http://arxiv.org/abs/2510.23595v1

- **作者**: Siqi Zhu; Yiding Wang; Haofei Yu; Jiaxuan You; Yixing Chen; Tao Feng

- **发布机构**: NVIDIA; Peking University; University of Illinois at Urbana-Champaign

---

# TL;DR
本文提出了一个名为 Multi-Agent Evolve (MAE) 的多智能体协同进化框架，通过实例化提议者（Proposer）、解决者（Solver）和评判者（Judge）三个角色，让大型语言模型在无需人类标注或可验证环境的情况下，通过强化学习自我提升解决数学、推理和通用问答等多样化任务的能力。

# 关键定义
本文的核心是其创新的框架和智能体角色设计，并未引入全新的术语，而是为现有概念赋予了新的协作机制：

*   **Multi-Agent Evolve (MAE)**: 一个多智能体自进化框架。它从单个基础大语言模型（LLM）中实例化出三个智能体角色，通过它们之间的协同与对抗互动，形成一个闭环的自我提升系统。
*   **Proposer (提议者)**: MAE 框架中的一个智能体角色，其任务是生成新的、具有一定挑战性的问题，以驱动解决者的学习进程。
*   **Solver (解决者)**: MAE 框架中的核心智能体角色，负责解答由提议者生成的问题。框架的主要目标就是提升该智能体的综合能力。
*   **Judge (评判者)**: MAE 框架中的评估智能体，采用 LLM-as-a-Judge 范式，负责评估提议者生成的问题质量和解决者生成的答案质量，并为它们提供数值奖励信号，引导整个系统的进化方向。

# 相关工作
当前，通过强化学习（Reinforcement Learning, RL）提升大语言模型（LLM）能力的研究取得了显著进展，但普遍存在两大瓶颈：
1.  **依赖人类标注数据**: 多数成功的 RL 方法依赖于昂贵且数量有限的人类精选数据集来提供可靠的奖励信号，这限制了其可扩展性。
2.  **依赖可验证环境**: 近期的自博弈（Self-Play）方法虽然减少了对人类数据的依赖，但通常需要一个可提供明确反馈的“可验证环境”，如 Python 解释器或游戏引擎。这使得它们难以推广到缺乏明确对错标准的开放域任务，如常识推理或通用知识问答。

本文旨在解决的核心问题是：**如何在没有人类标注和特定领域可验证环境的情况下，构建一个有效的通用强化学习框架，使 LLM 能够自我提升？**

# 本文方法

本文的核心方法是 Multi-Agent Evolve (MAE) 框架，它通过一个闭环的自博弈和自评估机制，驱动单个 LLM 不断进化。该框架由三个从同一基础 LLM 实例化的智能体角色构成：提议者（Proposer）、解决者（Solver）和评判者（Judge）。

<img src="/images/2510.23595v1/x1.jpg" alt="框架概览" style="width:85%; max-width:450px; margin:auto; display:block;">

MAE 从单个 LLM 中实例化出三个互动角色（提议者、解决者和评判者），形成一个闭环的自我提升循环。提议者生成新问题，解决者尝试回答，评判者对两者进行评估并提供通用领域的奖励信号。评判者奖励解决者准确的推理，而提议者则同时获得来自评判者的质量评分和在解决者失败时增加的难度奖励，从而形成一个对抗性的协同进化过程，持续增强模型的推理能力。

### 角色与奖励设计

#### 提议者 (Proposer)
提议者的目标是提出既可解又有挑战性的高质量问题。它接收生成指令 $$I_g$$，并可以选择性地参考一个现有问题 $$q_ref$$ 来生成新问题 $$q$$。




{% raw %}$$
q \sim \pi_P(\cdot  \mid  I_g, (q_{ref}); \theta)
$${% endraw %}



其奖励函数 $$R_P$$ 由三部分加权构成：
1.  **质量奖励 (Quality Reward) $R\_{quality}$**: 由评判者评估问题本身的清晰度和可解性后给出的分数。
2.  **难度奖励 (Difficulty Reward) $R\_{difficulty}$**: 旨在鼓励提议者生成对当前解决者有挑战性的问题。该奖励与解决者对该问题的平均得分成反比。
    

    {% raw %}$$
    \bar{R}_S(q) = \frac{1}{N_{sample}}\sum_{i=1}^{N_{sample}}V_J(a_i, q) \quad \text{where } a_i \sim \pi_S(\cdot \mid q)
    $${% endraw %}


    

    {% raw %}$$
    R_{difficulty}(q) = 1 - \bar{R}_S(q)
    $${% endraw %}


3.  **格式奖励 (Format Reward) $R\_{format}$**: 确保提议者生成的内容包含正确的 $$<question>$$ 标签，便于解析。

总奖励为：


{% raw %}$$
R_P(q) = \lambda_{quality}R_{quality} + \lambda_{difficulty}R_{difficulty} + \lambda_{format}R_{format}
$${% endraw %}



为保证训练稳定性，MAE 还引入了**质量过滤 (Quality Filtering)** 机制，即只有评判者评分高于特定阈值（如0.7）的问题才会被加入到有效问题池中。

#### 解决者 (Solver)
解决者的任务是为提议者提出的问题 $$q$$ 生成高质量的答案 $$a$$。


{% raw %}$$
a \sim \pi_S(\cdot \mid I_S, q; \theta)
$${% endraw %}



其奖励函数 $$R_S$$ 由两部分加权构成：
1.  **评判奖励 (Judge Reward) $R\_{judge}$**: 这是解决者的主要奖励来源。评判者评估答案 $$a$$ 的质量和正确性，并给出分数 $$V_J(a,q)$$。
2.  **格式奖励 (Format Reward) $R\_{format}$**: 确保其输出包含正确的 $$<answer>$$ 标签。

总奖励为：


{% raw %}$$
R_S(a) = \lambda_{judge}R_{judge} + \lambda_{format}R_{format}
$${% endraw %}



#### 评判者 (Judge)
评判者是一个生成式奖励模型，它通过生成详细的分析（Chain-of-Thought）来产出数值分数，从而指导提议者和解决者的训练。它执行两项评估任务：
1.  **评估答案**: 根据严格的评分标准（如事实错误、逻辑错误等）为解决者的答案打分。
2.  **评估问题**: 根据问题的可解性、逻辑一致性等为提议者的问题打分。

一个关键的创新点是，**评判者自身也需要被训练**。它会收到一个格式奖励 $$R_format$$，以激励其输出稳定、可解析的评分格式（例如，$$ <score>X</score> $$），这对于自动化整个自博弈循环至关重要。

### 协同训练流程

<img src="/images/2510.23595v1/x2.jpg" alt="训练机制" style="width:85%; max-width:600px; margin:auto; display:block;">

(上) MAE 使用骨干 LLM 自身作为问题和答案的通用评估器。(左下) 框架将质量过滤技术应用于提议者的生成循环，防止在长期训练中数据集质量下降。(右下) 多智能体训练采用 Task-Relative REINFORCE++，为每个角色分别计算优势函数，然后对统一模型执行同步参数更新。

整个训练流程如算法所示，在一个训练步骤中：
1.  **提议阶段**: 提议者生成一批新问题（可参考或不参考现有问题）。
2.  **过滤与奖励**: 评判者评估新问题的质量，合格的问题被加入问题池。提议者根据质量、难度和格式获得奖励。
3.  **解决阶段**: 解决者从问题池中抽取问题并生成答案，评判者对其答案进行评分。解决者根据评分和格式获得奖励。
4.  **评判阶段**: 评判者对其自身的评估输出格式获得奖励。
5.  **同步更新**: 使用 Task-Relative REINFORCE++ 算法，根据每个角色（提议者、解决者、评判者）各自获得的奖励计算梯度，并**同时更新共享的 LLM 模型参数**。

这种设计使得三个角色协同进化：提议者学会提出更有挑战性的问题，解决者学会更好地回答问题，而评判者则学会更准确地评估，形成一个正向反馈的自我提升循环。

``$$
算法：Multi-Agent Evolve 的训练工作流

输入：基础LLM M，种子问题数据集 D_0，训练步数 T，问题质量阈值 a
1. 从M初始化Proposer π_P, Solver π_S, Judge π_J
2. 初始化问题数据集 D ← D_0，问答对数据集 P ← ∅
3. for t = 1, ..., T do
4.     // 提议阶段 (可能带参考或不带参考)
5.     // ... 生成新问题 q_t
6.     // ... 计算难度奖励和质量奖励
7.     if R_quality(q_t) ≥ 0.7 then D ← D U {q_t} // 质量过滤
8.     // ... 计算提议者的总奖励 r_P
9.
10.    // 解决阶段
11.    从数据集 D 中采样问题 q
12.    生成答案 a_t ← π_S(I_S, q)
13.    P ← P U {(q, a_t)}
14.    // ... 计算解决者的总奖励 r_S
15.
16.    // 评判阶段
17.    从数据集 P 中采样 (q, a)
18.    生成评估 g ← π_J(q, a)
19.    // ... 计算评判者的格式奖励 r_J
20.
21.    // 更新阶段
22.    根据 r_P, r_S, r_J 更新 π_P, π_S, π_J 的共享参数
23. end for
24. 输出：从 π_P, π_S, π_J 聚合而成的改进模型 M'
$$`$$

# 实验结论

实验基于 $$Qwen2.5-3B-Instruct$$ 模型，在多种设定下评估了 MAE 框架的有效性。

### 主要发现

1.  **无参考问题下的进化**: 在仅使用极少量（16个）自生成问题作为初始种子的“零参考”设定下，MAE ($$MAE (zero)$$) 在绝大多数基准测试上都超越了基础模型。相较于同样不使用真实世界数据的 $$AZR$$ 基线，MAE 在整体平均分上更高（58.51 vs. 57.72），尤其在 $$AZR$$ 表现不佳的复杂推理任务（如 BBH, AMC）上取得了显著优势。这证明了 MAE 仅通过多角色协同进化就能有效提升通用能力。

2.  **有参考问题下的进化**: 当使用一个包含约1K个无标签参考问题的种子数据集时，MAE 的所有变体都显著优于在相同数据上使用有标签答案进行训练的监督微调（SFT）基线。值得注意的是，SFT 基线在此设定下甚至出现了性能退化（平均分 53.87 vs. 基础模型 55.33），而 MAE 则取得了显著提升。
    *   表现最佳的是 $$MAE (half reference)` 设置，它通过一半时间参考现有问题、一半时间从头创造新问题的方式，在探索（生成新类型问题）和利用（加深现有问题分布）之间取得了平衡。该设置在分布内（ID）和分布外（OOD）任务上均取得了最高平均分，**总体平均分达到 59.98%**，相较于基础模型提升了 4.54%。

### 训练稳定性分析

与一些以往的 LLM 交互方法容易出现训练崩溃的问题不同，MAE 展现了出色的训练稳定性。
*   模型性能在超过 250 个训练步骤（批量大小128）后仍在持续提升。
*   **训练曲线分析**: 如下图所示，在训练过程中，合格问题数据集的规模稳步增长，表明质量过滤机制有效防止了模型退化。同时，提议者逐渐学会生成对解决者有“适当难度”的问题（解决者的平均得分维持在一个有益于学习的中间水平），从而持续为模型进化提供有效的训练信号。

<img src="/images/2510.23595v1/x3.jpg" alt="训练过程分析" style="width:90%; max-width:700px; margin:auto; display:block;">

(左) 数据集中的问题数量稳定增加，同时低质量问题被排除。(中和右) 提议者学会生成对解决者具有理想难度级别的问题，从而在未来的训练中有益于模型。

### 实验结果表格

下表总结了在有/无参考问题设置下的主要实验结果。


| 模型 | GSM8K | MATH | ARC-C | MMLU | ... | ID 平均 | OOD 平均 | 总体平均 |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **无参考问题** | | | | | | | | |
| Base | 60.40 | 7.91 | 80.60 | 54.79 | ... | 66.07 | 40.52 | 55.33 |
| AZR (基线) | **64.50** | 7.91 | 80.70 | 56.40 | ... | 65.65 | 45.41 | 57.72 |
| MAE (zero) | 60.60 | **11.41** | **83.40** | **57.17** | ... | **66.69** | **46.06** | **58.51** |
| **有参考问题** | | | | | | | | |
| Base | 60.40 | 7.91 | 80.60 | 54.79 | ... | 66.07 | 40.52 | 55.33 |
| SFT (基线) | 59.20 | 7.61 | 82.20 | 54.06 | ... | 63.28 | 37.41 | 53.87 |
| MAE (no reference) | 65.10 | 10.31 | **84.40** | 56.54 | ... | 66.45 | 44.59 | 58.18 |
| MAE (with reference) | 65.70 | **11.11** | 83.70 | 56.63 | ... | 65.07 | 43.18 | 56.47 |
| MAE (half reference) | **66.50** | 10.51 | 84.10 | **59.20** | ... | **68.95** | **43.96** | **59.98** |

### 最终结论
实验结果有力地证明，MAE 是一个可扩展且数据高效的框架。它能够在最小化对人类标注数据依赖的情况下，通过多智能体协同进化有效提升 LLM 的通用推理能力，甚至在某些情况下优于使用真实答案进行监督微调的方法。