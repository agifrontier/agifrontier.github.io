---
layout: default
title: "Unsupervised Data Generation for Offline Reinforcement Learning: A Perspective from Model"
---

# TL;DR
本文从模型角度理论上分析了离线强化学习（Offline RL）中行为数据分布如何决定离线RL性能，并提出了一套无监督数据生成（UDG）方法，通过训练多样化智能体，生成多样化数据并选择最优数据用于下游任务，大幅提升了任务不可知情况下的离线RL表现。

# 关键定义

- **离线强化学习 (Offline Reinforcement Learning, Offline RL)**  
  指在不能与环境交互时，仅利用已收集的数据训练智能体的强化学习方法。

- **分布外问题 (Out-of-distribution Problem, OOD)**  
  指智能体对未在行为数据分布出现的样本估值过高，导致训练崩溃的问题。

- **行为策略 (Behavioral Policy, $\pi^\beta$)**  
  用于生成训练数据的策略，可能不是任务的最优策略。

- **状态-动作占据分布 (Occupancy Distribution, $\rho\_T^\pi(s, a)$)**  
  策略$\pi$在环境下经历的所有状态-动作对随时间加权的概率分布。

- **Wasserstein距离 (Wasserstein Distance, $W\_1$)**  
  衡量两个概率分布之间距离的常用数学工具，Wasserstein-1即欧氏距离度量下的最小运输成本。

# 相关工作

## 领域现状

离线RL因在线交互成本高，在安全要求强的现实问题中受到关注。主流进展聚焦于模型自由度、分布外样本抑制、以及如何通过约束策略或奖励修正提升泛化能力。  
模型无关方法多靠数据分布约束或价值抑制解决分布外估值问题，模型相关方法（如MOPO）则借助模型生成新样本，能提升“分布内”泛化，但若行为数据分布偏离最优策略，则仍有显著性能损失。

现有理论对数据分布与性能关系探讨有限，多数仅经验分析数据多样性提升任务泛化，但缺乏严格理论证明。

## 关键问题

- 如何理论化地刻画行为数据分布与离线RL性能的联系？
- 在任务未知（task-agnostic）或多任务场景，如何高效生成能适配后续未知任务的数据？
- 如何在数据生成过程中最小化对后续所有可能任务的性能损失（后悔, regret）？

# 本文方法

## 理论分析创新

作者以模型为视角，构建了行为数据分布与最终离线RL性能之间的数学桥梁，基于MOPO的下界理论展开分析，给出了如下核心结论：

- 离线RL智能体的最终回报距离最优策略回报的下界，直接受行为策略（$\pi^\beta$）与最优策略（$\pi^\*$）状态-动作分布的Wasserstein距离控制：

  


  $$
  \eta_M(\hat{\pi}) \geq \eta_M(\pi^*) - 2C W_1(\rho_T^{\pi^\beta}, \rho_T^{\pi^*})
  $$



- 若批数据据来自多个策略的混合，最优下界还与最接近最优策略的数据相关：

  


  $$
  \eta_M(\hat{\pi}) \geq \eta_M(\pi^*) - C\left(
    W_1(\rho_T^{\pi^\beta},\rho_T^{\pi^*}) + \min_i W_1(\rho_T^{\pi_i^\beta},\rho_T^*)
  \right)
  $$



- 理论上，当行为策略与最优策略分布接近，离线RL智能体性能也更接近最优。

## 无监督数据生成框架（UDG）

<img src="/images/2506.19643v1/udg01_v2.jpg" alt="UDG架构图" style="width:90%; max-width:800px; margin:auto; display:block;">

### 方法流程

1. **策略多样化训练**  
   同时训练 $K$ 个智能体，各自以“多样性”奖励为目标（具体为最小化与其他策略状态分布的Wasserstein距离）。

2. **数据采集**  
   每个智能体独立与环境交互，收集各自的经验，形成多个数据buffer。

3. **奖励重标记与数据选择**  
   在下游具体任务到来后，使用目标任务的奖励函数对已收集数据重标记；评估所有buffer，选择平均回报最高者。

4. **离线RL训练**  
   利用最优buffer数据，采用MOPO等方法训练最终智能体。

### 最小化最坏后悔理论

设计目标等价于




$$
\min_{\{\pi_i\}} \max_{\pi^*} \min_{i} W_1(\rho_T^{\pi_i}, \rho_T^{\pi^*})
$$



即寻找一组策略，使得无论未来出现什么任务，总有一个策略生成的数据与最优策略分布足够接近，从而保证离线RL性能下界较高。

为了实际优化，用“策略之间最小的Wasserstein距离最大化”作为代理目标：




$$
\max_{\{\pi_i\}} \min_{i \neq j} W_1(\rho_T^{\pi_i}, \rho_T^{\pi_j})
$$



该思想与WURL（Wasserstein Unsupervised RL）方法一致，通过奖励机制促进智能体探索多样化行为，实现数据多样性覆盖。

### 算法实现要点

- 多智能体采用SAC算法训练，奖励部分结合多样性目标；
- 多任务/部分奖励可通过不同比例结合已知任务奖励与多样性奖励实现；
- 下游数据选择采用“最大平均回报”原则保证性能与泛化。

# 实验结论

## 主要结果

- **Ant-Angle任务**  
  在六个朝向不同的运动任务中，UDG框架能用多样化buffer覆盖所有方向，无监督生成的数据显著优于“仅针对某方向”训练数据。  
  <img src="/images/2506.19643v1/udg03_v1.jpg" alt="Ant-Angle任务效果示意" style="width:80%; max-width:400px; margin:auto; display:block;">

- **Cheetah-Jump任务**  
  UDG生成的多样性数据buffer同样在奖励未知或变化的情况下大幅提升下游离线RL效果。  

| 任务 | 奖励权重$c\_z$ | 随机数据 | 多样性数据 |
| --- | --- | --- | --- |
| Cheetah-Jump | 15 | 1152.98±120 | 1721.25±56 |
| Cheetah-Crawl | -15 | 1239.00±57 | 1348.19±274 |

## 数据混合分析

- 单一最优buffer表现通常最好；混合多buffer时，过宽的数据分布反而可能降低下界，但合理选择“最接近期望分布的多个buffer”可提升泛化。

| 角度 | top 1 | top 2混合 | 全部混合 |
| --- | --- | --- | --- |
| 0° | 1236.26±247 | 1437.24±31 | 989.13±65 |
| 60° | 910.70±121 | 1285.31±66 | 593.88±434 |
| … | … | … | … |

## 结论与优点

- UDG框架通过理论指导的多样化探索，针对任务未知/多任务场景生成覆盖广且可选多样性数据。
- 选择最优回报的buffer极大缓解分布外问题，提升任务泛化与最坏场景性能下界。
- 理论与实证均表明：行为数据分布与最优策略分布的距离，决定了离线RL智能体的性能极限；UDG能系统性最小化此距离。

## 局限性与讨论

- 理论假设环境状态连续且满足Lipschitz条件，像Atari或目标稀疏任务不一定适用；
- 实践中采用非参数模型，神经模型泛化能力待进一步理论刻画；
- 多样性奖励必须在低维空间计算，需领域先验；
- 无监督探索阶段可能带来现实安全风险，部署需额外保障。

---

# 总结

本文提出的无监督数据生成（UDG）框架有力解决了离线强化学习在未知任务/多任务场景下的数据生成与选择难题，补充了理论与实践之间的断层，实现了更高的任务泛化与性能下界。核心思想为多样化智能体训练与后续最优数据选择，理论证明与实验结果高度一致，具有良好可扩展性和现实应用价值。