---
layout: default
title: "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research"
---

# AI自己当“考官”：DR Tulu-8B用进化标尺实现顶级研究，成本降低近1000倍

<img src="/images/2511.19399v1/A__title.jpg" alt="" style="width:85%; max-width:600px; margin:auto; display:block;">

当AI Agent被要求撰写一篇深度研究报告时，它不仅要规划、搜索、综合信息，还要给出可靠的引用。这正是“深度研究”（Deep Research）任务的难点所在。目前，多数开源模型难以胜任，而强大的闭源系统不仅是黑箱，而且成本高昂。

> ArXiv URL：http://arxiv.org/abs/2511.19399v1

现在，来自艾伦人工智能研究所（AI2）等顶尖机构的研究者们，推出了**DR Tulu-8B**。这是一个仅有80亿参数的开源模型，却在多项深度研究基准测试中，其表现不仅远超更大的开源模型，甚至媲美乃至超越了昂贵的专有系统，而每次查询成本降低了近1000倍！

这一切是如何实现的？答案在于一种全新的训练方法：**带进化标尺的强化学习**（**Reinforcement Learning with Evolving Rubrics, RLER**）。

### 深度研究的“考官难题”

训练一个深度研究AI，最大的挑战在于如何评价它生成的长篇答案。

一个好的答案没有固定标准。它可能在A方面很出色，但在B方面有欠缺。用一套静态、固定的评估标准（Rubrics）去打分，就像用一把尺子去衡量世间万物，根本无法捕捉复杂任务的精髓。

更糟糕的是，如果评估标准只依赖大模型自身的“ parametric knowledge”，模型很容易学会“钻空子”，通过迎合“考官”的偏好来获得高分，而不是真正提升研究能力。

### RLER：让“考官”与“考生”一同进化

为了解决这个难题，该研究提出了RLER方法。它的核心思想非常巧妙：**评估标准不再是静态的，而是与被训练的模型共同进化。**

![RLER 框架概览](images/page_1_Figure_0.jpg)

可以把它想象成一个动态的教学过程：

1.  **模型探索**：在训练的每一步，模型（“考生”）会针对一个问题生成多个不同的答案。

2.  **生成标尺**：系统会分析这些答案，并结合从外部搜索到的真实知识，动态生成一套新的评估标尺（Rubrics）。这些标尺会专门针对当前答案的优缺点进行评价。

3.  **反馈与学习**：模型根据这套“量身定制”的标尺获得奖励信号，并通过强化学习进行优化。

这个过程的关键在于，评估标尺是**基于外部知识的**（grounded）并且是**与策略同步的**（on-policy）。当模型探索到新的知识或产生新的行为时，“考官”会立刻调整自己的评分标准，从而提供最有效、最精准的反馈，引导模型向正确的方向探索和学习。

### DR Tulu-8B的诞生之路

构建DR Tulu-8B主要分为两个阶段：

**第一步：监督微调（SFT）解决冷启动。**

一个未经训练的基础模型连如何调用工具都不知道。因此，研究者首先使用一个强大的教师模型（GPT-4）生成了16000条高质量的深度研究轨迹数据，对基础模型（Qwen1.5-7B）进行SFT，教会它规划、工具使用和引用的基本功。

**第二步：RLER在线强化学习。**

在SFT的基础上，DR Tulu通过RLER进行在线学习。借助高效的**异步工具调用**（Asynchronous Tool Calling）架构，模型可以在训练中实时与搜索引擎、论文数据库等外部工具交互，不断探索并优化其研究策略。

### 惊人的性能与成本效益

DR Tulu-8B的表现堪称惊艳。在四个涵盖科学、医疗和通用领域的长篇深度研究基准上，它全面超越了现有开源模型，包括之前最强的30B模型。

![性能与成本对比](images/page_0_Figure_8.jpg)

更令人印象深刻的是它的成本效益。如上图所示，DR Tulu-8B处在性能-成本曲线的帕累托前沿。在ScholarQA-CSv2基准上，OpenAI Deep Research每次查询成本约为$1.8美元，而DR Tulu-8B仅需$0.0019美元，便宜了将近三个数量级！

研究者还在一个全新的、极具挑战性的真实世界任务——**遗传病致病基因变异研究**上测试了DR Tulu。结果显示，尽管其底层模型参数远小于GPT-4等专有模型，但在证据综合、证据质量和证据支持度等关键指标上，DR Tulu-8B甚至能够媲美或超越它们。

### 智能体：学会因地制宜

DR Tulu不仅仅是一个文本生成器，更是一个懂得根据任务需求自适应选择工具的智能Agent。

![不同任务下的工具使用分布](images/page_16_Figure_2.jpg)

分析显示，在处理科研文献类问题（如SQAv2）时，它会高频使用$$paper_search$$工具；而在面对通用领域问题（如DeepResearchBench）时，则更依赖$$google_search$$。这种自适应能力是实现高质量深度研究的关键。

### 总结

DR Tulu-8B的出现，为开源社区带来了第一个真正为长篇、开放式深度研究任务而训练的强大模型。其核心贡献RLER方法，通过让评估标准与模型共同进化，巧妙地解决了长篇生成任务中奖励信号难以定义的核心痛点。

更重要的是，该研究团队开源了所有数据、模型、代码，以及一个名为$$dr-agent-lib$$的Agent基础设施。这无疑将极大地推动开源深度研究Agent的发展，让更多研究者能在此基础上构建更强大、更高效的AI研究助手。