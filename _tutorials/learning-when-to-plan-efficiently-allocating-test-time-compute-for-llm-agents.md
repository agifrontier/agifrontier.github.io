---
layout: default
title: "Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents"
---

# Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents

- **ArXiv URL**: http://arxiv.org/abs/2509.03581v1

- **作者**: Jack Parker-Holder; Edward Grefenstette; Jonathan Cook; Tim Rocktäschel; Jens Tuyls; Bartłomiej Cupiał; Ulyana Piterbarg; Davide Paglieri

- **发布机构**: IDEAS NCBR; New York University; Princeton University; University College London; University of Oxford; University of Warsaw

---

# TL;DR
本文提出了一种两阶段（SFT+RL）训练方法，使大型语言模型（LLM）智能体能够学会在序贯决策任务中动态地决定何时进行规划，从而高效地分配测试时计算资源，实现更高的任务效率、性能和可控性。

# 关键定义
本文提出了一个用于动态规划的 conceptual framework (概念框架)，并沿用了一些强化学习领域的标准定义。以下是理解本文至关重要的核心概念：

1.  **动态规划 (Dynamic Planning)**: 指智能体在序贯决策任务中，根据当前状态和上下文，灵活地决定在每个时间步是否要分配额外的测试时计算资源 (test-time compute) 来生成或更新其行动计划。这与“总是规划”（如 ReAct）或“从不规划”的固定策略形成对比。

2.  **概念性策略分解 (Conceptual Policies Decomposition)**: 本文将一个单一、整合的LLM的行为概念性地分解为三个策略，这并非独立的模型组件，而是通过统一的生成格式实现：
    *   **决策策略 ($\phi\_{\theta}$)**: 决定当前是否需要规划 ($d\_t \in \{0, 1\}$)。这个决策是隐式的，通过模型是否生成 $$<plan>$$ 令牌来体现。
    *   **规划策略 ($\psi\_{\theta}$)**: 在决定规划时，生成一个新的自然语言计划 ($p\_t$)。
    *   **行动策略 ($\pi\_{\theta}$)**: 根据当前上下文和计划，生成最终要执行的动作 ($a\_t$)。

3.  **规划优势 (Planning Advantage, $A\_{plan}$)**: 生成一个新计划相对于沿用旧计划所能带来的预期未来回报的提升量。它量化了规划的潜在收益，定义为：


    {% raw %}$$
    A_{plan}(c_{t})=\mathbb{E}_{p_{t}\sim\psi_{\theta}(\cdot\mid c_{t},d_{t}=1)}[V^{\pi_{\theta}}(c_{t},p_{t})-V^{\pi_{\theta}}(c_{t},p_{t-1})]
    $${% endraw %}



4.  **规划成本 (Cost of Planning, $C\_{plan}$)**: 规划行为所产生的总成本，是智能体决策时需要权衡的负面因素。它主要包含三个部分：
    *   **计算成本 ($C\_{tokens}$)**: 生成计划所耗费的 token 数量带来的直接成本。
    *   **延迟成本 ($C\_{latency}$)**: 规划所花费的时间，在时间敏感任务中尤为重要。
    *   **不稳定性成本 ($C\_{noise}$)**: 一个概念性成本，指过于频繁或不一致的重规划可能导致行为不稳定（如目标摇摆、无效回溯），从而降低任务性能。

# 相关工作
当前，在LLM智能体领域，主流方法如 ReAct 倾向于在每一步行动前都进行思考或规划。这种“总是规划”的策略虽然在某些单步推理任务上表现出色，但在复杂的长时程序贯决策任务中，存在两个关键瓶颈：
1.  **计算成本高昂**：在每一步都生成规划会消耗大量的计算资源。
2.  **性能下降**：过于频繁的重规划会引入行为不稳定性，导致智能体在目标间摇摆不定或执行低效操作，反而降低了在长时程任务中的最终表现。

与此同时，“从不规划”的策略则限制了智能体解决复杂问题的能力。因此，现有研究在如何高效分配LLM的测试时计算资源上存在空白。

本文旨在解决的核心问题是：**如何让LLM智能体学会自适应地决定何时应该规划，从而在规划带来的性能提升与计算和不稳定性成本之间找到最佳平衡点？**

# 本文方法
本文提出了一个概念框架，并基于此设计了一套两阶段训练流程，旨在教会LLM智能体进行动态规划。

<img src="/images/2509.03581v1/diagram_v2.jpg" alt="Dynamic Planning Agent Architecture" style="width:90%; max-width:700px; margin:auto; display:block;">
<p align="center">图2：动态规划智能体架构。智能体是一个单一、整合的LLM，其概念性策略通过统一的输出格式实现。规划决策($\phi\_{\theta}$)由模型选择是否以 <plan> 令牌开始生成来隐式做出。这个单一输出字符串被解析以提取动作($a\_t$)以及（如果存在）新计划($p\_t$)，从而执行行动($\pi\_{\theta}$)和规划($\psi\_{\theta}$)策略。</p>

## 概念框架
本文首先建立了一个理论框架来形式化动态规划的成本效益权衡。智能体的行为被建模为在部分可观察马尔可夫决策过程 (Partially-Observable Markov Decision Process) 中的决策。一个单一的LLM通过其生成的内容，隐式地执行了决策、规划和行动三个策略。

### 核心思想：成本效益分析
智能体只应在规划的预期收益超过其成本时才进行规划。
*   **收益**：由**规划优势 ($A\_{plan}$)** 来衡量，即采纳新计划带来的期望回报增益。
*   **成本**：由**规划成本 ($C\_{plan}$)** 构成，包括了可直接惩罚的**计算成本 ($C\_{tokens}$)**，以及会间接导致任务回报降低的**不稳定性成本 ($C\_{noise}$)**。

### 训练目标
为了让智能体隐式地学会这种成本效益分析，本文采用强化学习进行微调，其优化目标是最大化任务回报与规划计算成本惩罚项之差的期望折扣总和。


在这个目标函数中，糟糕规划策略（如过于频繁的规划）导致的不稳定性会自然地降低任务回报 $R\_{task}$，因此也被隐式地惩罚。通过优化该目标，智能体能够联合学习何时规划、如何规划以及如何执行计划。

## 两阶段训练流程
为了在实践中实现上述目标，本文设计了一个包含监督微调预训练和强化学习优化的两阶段流程。

### 第一阶段：监督微tuning (SFT) 预训练 (Priming)
*   **目的**：为后续的RL阶段做准备，通过模仿学习向一个中等规模的模型（Llama-3.1-8B）注入多样化的规划行为和结构知识。
*   **方法**：
    1.  **数据生成**：使用一个更强大的“教师”模型（Llama-3.3-70B）在 Crafter 环境中生成1024条轨迹。为保证数据多样性，教师模型在每条轨迹中以随机的频率（每 $K$ 步，其中 $K$ 从[2, 12]中均匀采样）进行规划。
    2.  **SFT训练**：在生成的数据上对Llama-3.1-8B模型进行微调。关键在于训练目标的格式：如果教师模型在该步骤进行了规划，则学习目标是 $$<plan>...</plan> [Action]$$ 的格式；否则，学习目标仅为 $$[Action]$$。这直接教会了模型根据上下文决定是否输出规划。

### 第二阶段：强化学习 (RL) 微调
*   **目的**：在长时程环境中，进一步通过与环境的交互来优化和提炼动态规划的能力。
*   **方法**：使用近端策略优化 (Proximal Policy Optimization, PPO) 算法，对经过SFT预训练的智能体进行微调。RL的奖励函数直接使用上文提到的目标函数，即任务奖励减去规划的token成本。

### 创新点
*   **学习“何时”规划**：与以往固定的规划策略（总是/从不/固定频率）不同，本文的核心创新在于让智能体通过学习来获得一个动态的、依赖于状态的规划决策策略。
*   **隐式决策机制**：规划的决策不是由一个独立的模块做出，而是内嵌在LLM自身的生成过程中，通过是否生成特定的 $$<plan>$$ 令牌来隐式完成，架构简洁高效。
*   **SFT+RL两阶段范式**：证明了SFT预训练对于中等规模模型学习复杂的动态规划行为至关重要，它为RL阶段的有效优化提供了必要的“归纳偏置”。

# 实验结论

<img src="/images/2509.03581v1/main_figure_v20.jpg" alt="主要结果图" style="width:85%; max-width:450px; margin:auto; display:block;">
<p align="center">图1：不同环境和训练阶段的动态规划策略。(a-b) Zero-shot结果显示，在Crafter和POGS中存在“金发姑娘”般的最佳规划频率。(c-d) SFT结果表明，规划智能体在与基础模型的KL散度较低的情况下性能更好。(e-f) RL结果显示，经过SFT预训练的规划智能体比非规划基线样本效率更高，且能更稳定地达成复杂目标。</p>

本文通过在自定义的POGS环境和复杂的Crafter环境中的一系列实验，得出以下关键结论：

1.  **存在“金发姑娘”效应 (Goldilocks Effect)**：Zero-shot实验明确显示，无论是“总是规划”（类似ReAct）还是“从不规划”都不是最优策略。性能峰值出现在中等规划频率（例如，在Crafter中每4步规划一次），过于频繁的规划反而因引入不稳定性（如增加无效回溯）而导致性能下降。这验证了本文框架中“不稳定性成本”的概念。

2.  **SFT预训练中规划信息的价值**：在SFT阶段，使用包含显式自然语言计划的数据进行训练的$$Primed-Dynamic$$模型，其性能显著优于在完全相同的动作序列上、但移除了所有计划信息进行训练的$$Primed-Naive$$模型。这表明，计划本身作为一种解释性信息，能够简化模仿学习的目标，并起到有效的正则化作用。

3.  **RL微调验证了动态规划的有效性**：
    *   **SFT是关键**：$$SFT+RL plan dynamically$$智能体的表现远超未经SFT预训练的基线模型，证明SFT为中等规模模型学习规划能力提供了必要的基础。
    *   **样本效率和性能更优**：$$SFT+RL plan dynamically$$智能体在训练初期展现出更高的样本效率，并能更快地达成更复杂的成就（如在Crafter中收集石料、制作石镐等）。这表明动态规划策略在游戏前中期优势明显。
    *   **能力瓶颈**：尽管初期优势显著，但在游戏后期，所有智能体的性能都趋于平缓，表明它们遇到了相似的生存技能瓶颈（如获取食物和水），这可能需要更多的训练算力来克服。

4.  **动态规划增强了人类协作与可控性**：
    *   经过$$SFT+RL$$训练的动态规划智能体能够很好地理解并遵循人类提供的外部高级计划。
    *   在人类指导下，该智能体成功完成了整个Crafter任务（挖到钻石），达到了其自主探索时远未能企及的高度。这与未经RL训练的模型形成鲜明对比，后者要么存在严重的“指令遵循”问题，要么会因过度记忆而忽略新颖的计划。

<img src="/images/2509.03581v1/best_of_n_v2.jpg" alt="Best-of-N 比较" style="width:85%; max-width:600px; margin:auto; display:block;">
<p align="center">图5：Crafter上的Best-of-N (N=100) 比较。该图显示了每种方法在N次轨迹中最佳表现的成就与回合步数。人类协作（N=20）表现最强，其次是SFT+RL动态规划和SFT+RL无规划，基础RL基线落后。</p>

**最终结论**：本文的研究清晰地证明了，教会LLM智能体在需要时才规划，是一种比现有固定策略更有效、更高效的范式。提出的两阶段训练方法成功地让智能体学会了动态分配测试时计算资源，不仅提升了自主性能，还极大地增强了与人类协作的潜力和系统的可控性，为构建更强大、高效且安全的智能体系统开辟了新的道路。