---
layout: default
title: "Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting"
---

# Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting

- **ArXiv URL**: http://arxiv.org/abs/2510.18874v1

- **作者**: Karthik Narasimhan; Danqi Chen; Noam Razin; Howard Chen

- **发布机构**: Princeton University

---

# TL;DR
本文通过系统性比较发现，强化学习（RL）在后训练中比监督微调（SFT）更能有效缓解灾难性遗忘，其根本原因在于RL利用在线策略（on-policy）数据所产生的“模式寻求”（mode-seeking）特性，能够在学习新任务的同时更好地保留模型的已有知识模式。

# 关键定义
本文沿用了现有概念，并通过新的视角对其在语言模型后训练中的作用进行了深入剖析：
*   **灾难性遗忘 (Catastrophic forgetting)**：指模型在学习新任务时，其在先前任务上已经获得的能力显著退化的现象。在语言模型（LM）后训练中，这通常表现为在 instruction following, general knowledge, reasoning 等已有能力上的性能下降。
*   **在线策略数据 (On-policy data)**：指用于训练的数据是由当前正在优化的策略（模型）生成的。这是强化学习（RL）的核心特征，与监督微调（SFT）通常使用固定的、离线策略（off-policy）的数据集形成对比。本文的核心论点是，数据是否“在线”是决定遗忘程度的关键。
*   **模式寻求 (Mode-seeking) vs. 模式覆盖 (Mode-covering)**：这是描述两种KL散度（KL divergence）优化方向特性的术语。
    *   RL 对应反向KL散度最小化，具有“模式寻求”特性，倾向于将策略的概率质量集中在目标分布的某个高概率模式上。
    *   SFT 对应正向KL散度最小化，具有“模式覆盖”特性，倾向于扩展策略的分布以覆盖目标分布的所有模式。
    本文反直觉地指出，在多模态策略下，RL的“模式寻求”特性更有利于防止遗忘。
*   **增益 ($\Delta\_{g}$) 与下降 ($\Delta\_{d}$)**：本文用于衡量后训练效果的两个核心指标。
    *   **增益 (Gain)**：模型在目标任务上的准确率提升，$\Delta\_{g} := \mathcal{A}(\pi\_{\theta\_T}, \mathcal{T}) - \mathcal{A}(\pi\_{\theta\_0}, \mathcal{T})$。
    *   **下降 (Drop)**：模型在非目标（保留）任务上的平均准确率下降，$\Delta\_{d} := \frac{1}{M}\sum\_{j=1}^{M} \mathcal{A}(\pi\_{\theta\_0}, \mathcal{T}'\_j) - \mathcal{A}(\pi\_{\theta\_T}, \mathcal{T}'\_j)$。理想的后训练应实现高增益和低下降。

# 相关工作
*   **研究现状**：通过监督微调（SFT）或强化学习（RL）等方法对预训练语言模型进行后训练，以适应特定任务或与人类偏好对齐，是当前的主流做法。然而，这一过程常常伴随着“灾难性遗忘”或被称为“对齐税（alignment tax）”的现象，即模型在获得新能力的同时，其原有的通用知识、推理或安全能力会受到损害。尽管已有研究观察到此现象，但对于不同后训练方法（特别是SFT和RL）在遗忘特性上的系统性差异及其根本原因，学界的理解仍然有限。
*   **本文解决的问题**：本文旨在解决以下具体问题：
    1.  SFT和RL在导致灾难性遗忘方面有何不同？哪种方法更具鲁棒性？
    2.  造成这种差异的根本机制是什么？
    3.  基于这一机制，能否为缓解大语言模型后训练中的遗忘问题提供实践指导？

# 本文方法

本文的核心论证分为三步：首先通过大量实验证明RL比SFT更能缓解遗忘；然后通过一个简化的多模态分布模型，从理论上解释了为何RL的“模式寻求”特性（源于其在线策略数据）能够更好地保留旧知识；最后通过消融实验验证了这一假设，并提出了更高效的实践建议。

## 实验对比：RL比SFT遗忘更少

本文首先对SFT和RL的遗忘模式进行了广泛的实证比较。

*   **实验设置**：
    *   **模型**：覆盖不同系列和规模的 Llama-3 和 Qwen-2.5 模型。
    *   **任务**：涵盖指令遵循 (IFEval)、通用知识 (MMLU) และ算术推理 (Countdown) 三种目标任务。
    *   **对比方法**：比较了三种方法：
        1.  **SFT**：使用由更强模型 (Llama-3.3-70B-Instruct) 生成的离线数据进行微调。
        2.  **Self-SFT**：使用模型初始策略生成的数据进行微调，是一种常见的自学习SFT。
        3.  **RL**：使用 GRPO 算法进行强化学习。
*   **实验发现**：
    *   如下图所示，在所有模型、数据集和任务上，RL（GRPO）均能在实现可比或更高目标任务增益（Gain）的同时，造成显著更低的非目标任务下降（Drop）。
    *   相比之下，两种SFT方法都表现出明显的权衡：为了在目标任务上获得高增益，通常不得不接受在其他任务上的严重性能下降。特别是SFT，虽然在指令遵循任务上获得了最高增益，但也付出了最大的遗忘代价。

<img src="/images/2510.18874v1/x2.jpg" alt="实验结果对比" style="width:85%; max-width:450px; margin:auto; display:block;">
> 对比了三种方法在不同模型和数据集上的增益（Gain，实心条）与下降（Drop，阴影条）。RL (GRPO) 在保持低Drop的同时实现了高Gain。

*   研究还发现，SFT方法对学习率等超参数很敏感。高学习率虽然能快速提升目标任务性能，但会导致严重遗忘；而低学习率虽然能缓解遗忘，却难以达到理想的目标性能，即使增加训练轮次也无济于事。

<img src="/images/2510.18874v1/x3.jpg" alt="SFT超参数影响" style="width:90%; max-width:700px; margin:auto; display:block;">
> Self-SFT 在不同学习率和训练轮次下的表现，展示了目标性能与遗忘之间的权衡。

## 理论解释：KL散度与多模态策略

为了解释RL为何能更好地缓解遗忘，本文构建了一个简化的理论模型，核心思想是分析模型策略在多模态分布下的更新动态。

<img src="/images/2510.18874v1/x1.jpg" alt="遗忘动态图示" style="width:85%; max-width:600px; margin:auto; display:block;">
> 遗忘动态图示。顶部（SFT/正向KL）：为了覆盖新任务模式，策略从旧知识模式中转移了概率质量，导致遗忘。底部（RL/反向KL）：策略通过移动自身的新模式来匹配目标，而旧模式基本保持不变。

*   **KL散度视角**：
    *   SFT 优化交叉熵损失，等价于最小化**正向KL散度** $$KL(π* || π_θ)$$，具有“模式覆盖”特性。
    *   RL 优化KL正则化的奖励，等价于最小化**反向KL散度** $$KL(π_θ || π*)$$，具有“模式寻求”特性。

*   **反直觉现象的解释**：
    传统观点认为，“模式寻求”更容易导致遗忘（因为它会放弃一些模式），而“模式覆盖”应该能更好地保留所有模式。本文的实验结果与此相悖。本文通过模拟实验揭示，初始策略的模态结构是关键。
    1.  **单模态策略模拟**：如果假设训练策略是单峰的，那么正向KL（SFT）确实比反向KL（RL）遗忘得更少。这符合传统直觉。
    <img src="/images/2510.18874v1/x4.jpg" alt="单模态模拟" style="width:90%; max-width:700px; margin:auto; display:block;">
    2.  **多模态策略模拟**：然而，对于像大语言模型这样复杂的系统，其策略更可能是多模态的（包含多种知识和能力）。在这种更现实的设定下，模拟结果截然相反：
        *   **正向KL (SFT)**：为了覆盖新的目标模式，它会“拉伸”整个概率分布，导致原本代表旧知识的模式的概率质量被稀释或转移，从而引发遗忘。
        *   **反向KL (RL)**：它倾向于移动策略中与新任务最接近的那个模式去匹配目标，而其他代表旧知识的模式则基本不受影响。

    <img src="/images/2510.18874v1/x5.jpg" alt="多模态模拟" style="width:90%; max-width:700px; margin:auto; display:block;">
    > 多模态设置下，反向KL（右图）通过移动一个模式来学习新任务，同时保持旧模式完整；而正向KL（左图和中图）则会破坏旧模式。

    这个发现是本文的核心洞察：在多模态策略下，RL源于在线策略数据的“模式寻求”行为，反而成为其缓解遗忘的关键优势。

## 验证与实践：在线策略数据是关键

本文通过一系列消融实验，验证了“在线策略数据”是RL缓解遗忘的主要原因，并探索了更具实践价值的“近似在线策略”方法。

### 创新点：在线策略数据是主因

RL与SFT有三个主要区别：(i) 在线策略数据 vs. 离线策略数据；(ii) KL正则化；(iii) 优势函数估计。实验旨在排除 (ii) 和 (iii) 的影响。

*   **KL正则化并非主因**：移除GRPO算法中的KL正则项后，模型依然保持了与原始GRPO相似的高增益、低下降特性。这表明KL正则化不是缓解遗忘的关键。

<img src="/images/2510.18874v1/x6.jpg" alt="KL正则化消融实验" style="width:90%; max-width:700px; margin:auto; display:block;">
> 对比有无KL正则化的GRPO，两者在增益-下降权衡上表现相似。

*   **优势函数估计并非主因**：将GRPO与不使用优势函数估计的经典RL算法REINFORCE进行比较。结果显示，REINFORCE虽然在提升目标任务性能上不如GRPO，但其遗忘水平同样很低。这说明优势函数等算法细节主要影响学习效率，而缓解遗忘的共性在于它们都使用了在线策略数据。

<br>


| 方法 | 模型 | 任务 | 增益 ($\Delta\_g$) | 下降 ($\Delta\_d$) |
| :--- | :--- | :--- | :--- | :--- |
| SFT | Llama-3.1-8B | IFEval | 23.33 | 14.50 |
| REINFORCE | Llama-3.1-8B | IFEval | 7.78 | 0.82 |
| GRPO | Llama-3.1-8B | IFEval | 14.89 | 1.10 |
| SFT | Llama-3.1-8B | MMLU | 15.68 | 8.87 |
| REINFORCE | Llama-3.1-8B | MMLU | 7.95 | 0.58 |
| GRPO | Llama-3.1-8B | MMLU | 9.09 | 0.44 |

<br>

### 优点：近似在线策略数据即可奏效

完全的在线策略学习（每步都重新生成数据）计算成本高昂。本文探索了更高效的“近似在线策略”方法。

*   **方法**：**Iterative-SFT**，即在每个训练 epoch 开始时，使用当前模型重新生成一次训练数据，再进行SFT。
*   **结果**：如下图所示，Iterative-SFT 能够在达到与SFT相当甚至更高目标任务准确率的同时，其遗忘程度（Drop）远低于SFT和Self-SFT，非常接近于完全在线的RL（GRPO）方法。

<img src="/images/2510.18874v1/x7.jpg" alt="近似在线策略数据实验" style="width:85%; max-width:600px; margin:auto; display:block;">
> 对比三种SFT变体。Iterative-SFT（迭代SFT）使用每轮开始时生成的近似在线数据，其遗忘程度（底部图表，接近0）远小于使用完全离线数据的SFT和Self-SFT。

这一发现具有重要的实践意义：无需完全采用RL的复杂框架，仅通过在SFT流程中引入周期性的数据再生成，就可以在很大程度上缓解灾难性遗忘。

# 实验结论
*   **核心结论**：本文通过全面的实验和理论分析证明，强化学习（RL）在语言模型后训练中比监督微调（SFT）更能有效缓解灾难性遗忘。
*   **关键原因**：这种优势并非来自KL正则化或特定的优势函数设计，而根本上源于RL对**在线策略数据（on-policy data）** 的使用。这种机制使得RL在优化时表现出“模式寻求”特性，在被视为多模态的语言模型策略中，这有助于在学习新任务的同时，保护已有的知识模式不受干扰。
*   **最佳表现**：RL方法（如GRPO）在所有实验中都展现了最佳的“高增益-低下降”权衡，能够在提升目标任务性能的同时，最大程度地保留非目标任务的能力。
*   **实践启示**：完全的在线策略学习计算开销大，但本文发现，采用**近似在线策略数据**（例如，在每个epoch开始时重新生成数据的Iterative-SFT）也足以显著减轻遗忘。这为在实践中平衡性能、成本和模型能力保持提供了一条有效且高效的途径。