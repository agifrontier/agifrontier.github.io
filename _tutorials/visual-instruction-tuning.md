---
layout: default
title: "Visual Instruction Tuning"
---

# Visual Instruction Tuning

- **ArXiv URL**: http://arxiv.org/abs/2304.08485v2

- **作者**: Haotian Liu; Chunyuan Li; Qingyang Wu; Yong Jae Lee

- **发布机构**: Columbia University; Microsoft Research; University of Wisconsin–Madison

---

# TL;DR
本文首次提出了视觉指令微调 (Visual Instruction Tuning) 的概念，通过使用纯语言的GPT-4生成高质量的图文指令遵循数据，并基于此数据训练了一个端到端的大型多模态模型 LLaVA，使其能够理解并执行通用的视觉和语言指令。

# 关键定义
*   **视觉指令微调 (Visual Instruction Tuning)**：一种扩展LLM指令微调到多模态领域的新方法。其核心思想是，使用机器生成的、包含图像和语言的指令遵循数据，对大型多模态模型 (LMM) 进行微调，以增强模型遵循人类意图、完成各种视觉相关任务的能力。这与旨在提高参数效率的视觉提示微调 (visual prompt tuning) 不同。
*   **LLaVA (Large Language and Vision Assistant)**：本文提出的端到-端训练的大型多模态模型。它通过一个简单的线性投射层，将预训练的CLIP视觉编码器与一个大型语言模型 (Vicuna) 连接起来，并在本文生成的视觉指令数据上进行微调。
*   **GPT辅助数据生成管道**：本文提出的一种自动化数据生成流程。该流程利用纯语言的GPT-4，将已有的图像-文本对（例如来自COCO数据集）转换为多样化的指令遵循数据。它通过向GPT-4提供图像的符号化表示（如描述文本和物体边界框），来生成三种类型的问答数据：对话、详细描述和复杂推理。

# 相关工作
当前，构建能够遵循指令的多模态智能体主要有两种路径：一是为特定任务（如导航、图像编辑）分别训练的端到端模型；二是通过LangChain等框架协调多个专用模型。这些方法要么任务范围狭窄，要么不是端到端训练。

另一方面，大型语言模型 (LLM) 如GPT-3、LLaMA等通过指令微调展现了强大的通用任务处理能力，但这些工作主要局限于文本领域。虽然已有Flamingo、BLIP-2等多模态模型，但它们并未经过视觉-语言指令的显式微调，在遵循指令进行多模态交互方面的能力相对较弱。

本文旨在解决的核心问题是：如何构建一个端到端训练的、能够像GPT-4一样遵循人类意图、处理多样化视觉任务的通用多模态助手。其主要瓶颈在于缺乏大规模、高质量的视觉-语言指令遵循数据。

# 本文方法
## GPT辅助的视觉指令数据生成
本文的核心创新之一是提出了一种利用纯语言大模型 (GPT-4/ChatGPT) 生成多模态指令遵循数据的方法，解决了该领域数据稀缺的关键问题。

该方法不直接将图片输入给GPT-4，而是将图片信息进行**符号化表示 (symbolic representations)**，转换成GPT-4可以理解的文本序列。具体使用了两种符号：
1.  **描述 (Captions)**：从不同角度描述图片内容的文字。
2.  **边界框 (Bounding boxes)**：定位图中物体，并编码其概念和空间位置的文本。

基于这些符号化表示和少量人工设计的种子示例（in-context-learning），GPT-4可以生成三种类型的指令数据：
*   **对话 (Conversation)**：模拟用户与助手之间关于图像内容的多轮问答。
*   **详细描述 (Detailed description)**：根据特定指令，生成对图像全面而丰富的描述。
*   **复杂推理 (Complex reasoning)**：基于图像内容，提出需要多步逻辑推理才能回答的深度问题。

最终，本文使用该流程在COCO数据集上共生成了158K条高质量的图文指令样本。


|  |  |
| --- | --- |
| **上下文类型1：描述** | |
| 一群人站在一辆黑色汽车外，旁边有各种行李。 | <img src="/images/2304.08485v2/sample_bench_fridge.jpg" alt="图片" style="width:85%; max-width:600px; margin:auto; display:block;"> |
| 行李围绕着一辆停在地下停车场的车。 | *(注：图片仅为参考，未输入给GPT)* |
| **上下文类型2：边界框** | |
| person: [0.681, 0.242, 0.774, 0.694], backpack: [0.384, 0.696, 0.485, 0.914], suitcase: … | |
| **响应类型1：对话** | |
| 问：图中是什么类型的车？ | |
| 答：图中是一辆黑色的运动型多功能车(SUV)… | |
| **响应类型2：详细描述** | |
| 该图像是一个地下停车场，停着一辆黑色的运动型多功能车(SUV)。场景中有三个人… | |
| **响应类型3：复杂推理** | |
| 问：这些人面临什么挑战？ | |
| 答：在图中，一群人正面临着将所有行李装入黑色SUV的挑战。有多个行李箱和背包需要打包… | |

*表 1：指令遵循数据生成示例。顶部为输入给GPT的上下文，底部为三种生成响应。*

## LLaVA 架构与训练
### 架构
LLaVA的架构简洁高效，其目标是有效地结合预训练视觉模型和LLM的能力。

<img src="/images/2304.08485v2/x1.jpg" alt="LLaVA 网络架构" style="width:90%; max-width:700px; margin:auto; display:block;">
*图 1: LLaVA 网络架构。*

1.  **视觉编码器**：使用预训练的CLIP ViT-L/14模型，将输入图像 ${{\bf X}}\_{\texttt{v}}$ 编码为视觉特征 ${\bf Z}\_{\texttt{v}} = g({{\bf X}}\_{\texttt{v}})$。
2.  **语言模型**：选用在语言任务上指令遵循能力出色的开源模型Vicuna作为基础LLM。
3.  **连接模块**：通过一个简单的**可训练线性投影矩阵 ${{\bf W}}$**，将视觉特征 ${\bf Z}\_{\texttt{v}}$ 映射到与LLM词嵌入空间维度相同的语言嵌入Token ${\bf H}\_{\texttt{v}}$。


{% raw %}$$
\mathbf{H}_{\texttt{v}} = \mathbf{W} \cdot \mathbf{Z}_{\texttt{v}}, \quad \text{with} \quad \mathbf{Z}_{\texttt{v}} = g(\mathbf{X}_{\texttt{v}})
$${% endraw %}


这种轻量级的设计使得模型可以快速进行以数据为中心的迭代实验。

### 训练
本文采用一个两阶段的指令微调过程：


| |
| :-- |
| ${{\bf X}}\_{\texttt{system-message}}$ <STOP> |
| $\texttt{Human}:{{\bf X}}\_{\texttt{instruct}}^{1}$ <STOP> |
| Assistant: ${\color[rgb]{0.234,0.707,0.293}\bf{X}}\_{\texttt{a}}^{1}$ <STOP> |
| $\texttt{Human}:{{\bf X}}\_{\texttt{instruct}}^{2}$ <STOP> |
| Assistant: ${\color[rgb]{0.234,0.707,0.293}\bf{X}}\_{\texttt{a}}^{2}$ <STOP> $\cdots$ |

*表 2：模型训练的输入序列格式。模型仅在预测助手回答（绿色部分）时计算损失。*

#### 阶段一：特征对齐预训练
该阶段旨在将视觉特征与LLM的词嵌入对齐。
*   **数据**：使用从CC3M数据集中筛选出的595K个图像-文本对，并将其简单转换为“提问-回答”格式（例如，问题是“简要描述这张图片”，答案是原始的图片标题）。
*   **训练**：冻结视觉编码器和LLM的权重，**仅训练投影矩阵 ${{\bf W}}$**。这一步可以看作是为冻结的LLM训练一个兼容的“视觉分词器 (visual tokenizer)”。

#### 阶段二：端到端微调
该阶段旨在教会模型遵循更复杂的指令。
*   **数据**：使用前述生成的158K条高质量多模态指令数据进行微调（用于多模态聊天机器人），或使用ScienceQA等特定任务数据。
*   **训练**：保持视觉编码器权重冻结，**同时更新投影矩阵 ${{\bf W}}$ 和LLM的权重 $\boldsymbol{\phi}$**。模型的目标是最大化在给定图像和指令的条件下，生成正确答案序列的概率，采用标准的自回归训练目标。

# 实验结论
## 多模态聊天机器人
### 定性评估
通过与多模态GPT-4、BLIP-2和OpenFlamingo进行比较，LLaVA展现了出色的对话和推理能力。如下图例所示，对于“这张图片有什么不寻常之处？”这类问题，LLaVA能准确理解指令意图并给出合理的解释，而其他模型则倾向于简单描述图像内容。


| <img src="/images/2304.08485v2/img_extreme_ironing.jpg" alt="极限熨烫图片" style="width:85%; max-width:450px; margin:auto; display:block;"> *来源：Barnorama* |
| :---: |
| **用户**：这张图片有什么不寻常之处？ |
| **LLaVA**：这张图片不寻常的地方在于，一名男子正在一辆小型货车或面包车的后部熨烫衣物。这并非进行此项活动的典型场所... |
| **GPT-4**：这张图片不寻常之处在于，一个男人正在一辆行驶的出租车车顶上，用一个固定在车顶的烫衣板熨烫衣服。 |
| **BLIP-2**：一个男人正坐在一辆黄色出租车的后部。 |
| **OpenFlamingo**：那个男人正在他的车盖上晾衣服。 |

*表 3：与 GPT-4、BLIP-2 和 OpenFlamingo 的聊天能力比较。LLaVA 能更好地遵循用户指令，而不是简单描述。*

### 定量评估
本文提出了LLaVA-Bench基准，并使用GPT-4作为评估者，对模型生成的回答进行打分（1-10分），并与一个理想情况下的参考模型（使用真实图像描述文本的GPT-4）进行比较，得出相对分数。

*   **LLaVA-Bench (COCO)**：消融实验表明，同时使用对话、详细描述和复杂推理三种指令数据进行训练效果最好，相对GPT-4参考模型取得了85.1%的分数。仅使用单一类型数据或缺少指令微调都会导致性能大幅下降。


| 训练数据 | 对话 | 详细描述 | 复杂推理 | 总计 |
| :--- | :---: | :---: | :---: | :---: |
| **完整数据** | **83.1** | **75.3** | **96.5** | **85.1** |
| 对话 + 5%详细描述 + 10%复杂推理 | 81.0 (-2.1) | 68.4 (-7.1) | 91.5 (-5.0) | 80.5 (-4.4) |
| 仅对话 | 76.5 (-6.6) | 59.8 (-16.2) | 84.9 (-12.4) | 73.8 (-11.3) |
| **无指令微调** | 22.0 (-61.1) | 24.0 (-51.3) | 18.5 (-78.0) | 21.5 (-63.6) |

*表 4：在LLaVA-Bench (COCO)上关于不同训练数据的消融研究。*

*   **LLaVA-Bench (In-the-Wild)**：在一个包含更多样化、更具挑战性图像的基准上，LLaVA的表现显著优于BLIP-2 (+29%) 和OpenFlamingo (+48%)，再次证明了视觉指令微调的有效性。


| 模型 | 对话 | 详细描述 | 复杂推理 | 总计 |
| :--- | :---: | :---: | :---: | :---: |
| OpenFlamingo | 19.3 | 19.0 | 19.1 | 19.1 |
| BLIP-2 | 54.6 | 29.1 | 32.9 | 38.1 |
| **LLaVA** | **57.3** | **52.5** | **81.7** | **67.3** |

*表 5：在LLaVA-Bench (In-the-Wild)上的指令遵循能力比较。*

### 局限性
实验也揭示了LLaVA的局限性，例如在处理需要OCR或细粒度识别的高分辨率图像时会遇到困难。有时模型会将图像感知为“补丁袋 (bag of patches)”，无法理解物体间的复杂语义关系（例如，将冰箱里的“草莓”和“酸奶”错误地组合成“草莓味酸奶”）。


| <img src="/images/2304.08485v2/sample_bench_ramen.jpg" alt="拉面图片" style="width:80%; max-width:300px; margin:auto; display:block;"> | <img src="/images/2304.08485v2/sample_bench_fridge.jpg" alt="冰箱图片" style="width:85%; max-width:600px; margin:auto; display:block;"> |
| :---: | :---: |
| 一兰拉面 | 装满食物的冰箱 |

*表 6：LLaVA-Bench（野外）中的挑战性示例，需要模型具备知识广度和细粒度识别能力。*

## ScienceQA
在ScienceQA这一大规模多模态科学问答基准上：
*   LLaVA本身取得了90.92%的准确率，接近当时的SOTA（91.68%）。
*   本文提出了一种新颖的集成策略：当LLaVA和纯文本GPT-4的答案不一致时，让GPT-4作为“裁判 (judge)”进行最终决策。这种方法出人意料地将准确率提升至**92.53%**，创造了新的SOTA。这表明，即使是无法看到图像的纯文本LLM，也能通过其强大的推理能力对多模态模型的输出进行修正和增强。


| 方法 | 平均准确率 (%) |
| :--- | :---: |
| GPT-3.5 w/ CoT | 75.17 |
| MM-CoT (之前SOTA) | 91.68 |
| GPT-4 (纯文本, 2-shot) | 82.69 |
| **LLaVA (本文方法)** | **90.92** |
| **LLaVA + GPT-4 (裁判)** | **92.53** |

*表 7：在ScienceQA测试集上的准确率。*

## 设计选择消融
*   **视觉特征**：使用CLIP倒数第二层特征比最后一层效果更好，可能因为它保留了更多局部细节。
*   **思维链 (CoT)**：采用“先推理后回答”的策略可以显著加速收敛，但对最终性能提升有限。
*   **预训练**：跳过第一阶段的预训练会导致性能大幅下降5.11%，证明了特征对齐的必要性。
*   **模型规模**：使用13B参数的模型比7B模型性能更好，显示了模型规模的重要性。