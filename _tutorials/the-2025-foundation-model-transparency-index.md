---
layout: default
title: "The 2025 Foundation Model Transparency Index"
---

# 2025 AI透明度大倒退：均分跌至40，IBM夺冠，xAI与Midjourney垫底

<img src="/images/2512.10169v1/A__title.jpg" alt="" style="width:85%; max-width:450px; margin:auto; display:block;">

随着AI模型的能力以惊人的速度进化，我们对这些“黑盒”背后的运作机制却似乎知之甚少。斯坦福大学等机构最新发布的 **2025年基础模型透明度指数**（**Foundation Model Transparency Index, FMTI**）揭示了一个令人担忧的趋势：尽管AI技术在飞速发展，但行业整体的透明度却在大幅倒退。

> ArXiv URL：http://arxiv.org/abs/2512.10169v1

这份年度重磅报告不仅对OpenAI、Google等老牌巨头进行了“体检”，还首次将阿里巴巴、DeepSeek等中国公司纳入评测范围。结果令人咋舌：平均分从去年的58分暴跌至40分，甚至低于2023年的水平。

### 透明度“寒冬”：谁在裸泳，谁在领跑？

今年的FMTI报告评估了13家全球顶级的基础模型开发商。研究团队设计了包含100项指标的评估体系，涵盖了从上游数据、模型构建到下游影响的全过程。

<img src="/images/2512.10169v1/x7.jpg" alt="Refer to caption" style="width:85%; max-width:600px; margin:auto; display:block;">

**红榜与黑榜的巨大反差：**

*   **冠军（IBM）**：IBM以 **95/100** 的高分一骑绝尘，成为透明度的绝对标杆。它在很多其他公司讳莫如深的领域（如数据来源、计算资源）都做到了充分披露。

*   **垫底（xAI & Midjourney）**：Elon Musk旗下的xAI和绘图模型巨头Midjourney仅获得 **14分**，处于极度不透明的状态。

*   **“中庸”的巨头们**：包括OpenAI、Google、Anthropic、Amazon和Meta在内的“前沿模型论坛”（Frontier Model Forum）成员，分数全部挤在中间梯队（平均约36分）。报告犀利地指出，这些公司似乎达成了一种默契——既避免因分数过低而声誉受损，又缺乏动力去争当透明度的领头羊。

**中国公司的首秀：**

今年首次参评的中国公司表现各异。阿里巴巴、DeepSeek等公司被纳入评估，虽然整体得分处于中下游（DeepSeek、Meta和Alibaba的平均分为30分），但这标志着全球AI透明度评估版图的完整化。

### 评分暴跌背后的真相：标准升级与刻意隐瞒

为什么今年的平均分会从58分跌至40分？这不仅仅是因为加入了得分较低的新公司，更是因为许多老牌玩家在关键指标上出现了“倒退”。

**1. 核心资源的“黑盒化”**

公司们对“上游资源”最为保密。训练数据（Training Data）和训练算力（Training Compute）是两个最大的黑洞。

*   **数据来源**：几乎没有公司愿意详细披露其训练数据的具体来源和构成，这直接关系到版权和偏见问题。

*   **算力成本**：尽管外界对训练大模型的昂贵成本充满好奇，但具体使用了多少 $FLOPs$、消耗了多少能源，往往被视为商业机密。例如，AI21 Labs在2024年还披露了算力和碳排放数据，但在2025年却选择了隐瞒。

**2. 评估标准的“硬核”升级**

FMTI 2025对指标进行了大幅修订，旨在“去伪存真”。

*   **拒绝模糊描述**：以前只要描述了模型能力（如“文本生成”）就能得分，现在必须列出“在后训练阶段专门优化的能力清单”。

*   **强调可复现性**：仅仅声称模型在某个基准测试上得分很高是不够的，必须开源代码和提示词（Prompts），证明第三方可以复现这一结果才能得分。

<img src="/images/2512.10169v1/x2.jpg" alt="Refer to caption" style="width:80%; max-width:300px; margin:auto; display:block;">

### 技术拆解：如何量化透明度？

为了科学地衡量透明度，研究团队将100个指标分为三个核心领域：

1.  **上游（Upstream）**：关注构建模型所需的资源。

    *   **数据**：数据源、版权、许可、PII（个人身份信息）处理。

    *   **劳动力**：涉及数据标注工人的薪资和工作环境。

    *   **计算**：硬件详情、能源消耗。

2.  **模型（Model）**：关注模型本身的属性和发布。

    *   **架构**：参数量、层数等（很多公司现在对此闭口不谈）。

    *   **能力与风险**：模型能做什么，不能做什么，以及潜在的安全隐患。

3.  **下游（Downstream）**：关注模型的使用和影响。

    *   **分发**：谁在使用模型？

    *   **影响**：对用户、受影响群体以及环境的实际影响。

<img src="/images/2512.10169v1/x1.jpg" alt="Refer to caption" style="width:90%; max-width:700px; margin:auto; display:block;">

### 有趣的发现：AI Agent能取代人类评估员吗？

在今年的评估过程中，研究团队进行了一项有趣的实验：利用AI Agent来辅助收集各公司的透明度信息。

结果显示，AI Agent确实能提高信息检索的效率，但还远不能完全取代人类。Agent容易产生“幻觉”或被表面信息误导（False Positives），同时也容易漏掉深藏在技术文档中的关键细节（False Negatives）。最终，所有信息仍需经过FMTI团队的人工核实。

### 结论：透明度是一种选择，而非技术难题

2025 FMTI报告最核心的启示在于，**透明度的差异主要源于企业意愿，而非技术或结构性障碍**。

IBM、Writer和AI21 Labs的高分证明，即使是商业化公司，也可以在保持竞争力的同时实现高度透明。相反，某些公司在下游应用政策（如下载使用条款）上得分极高，却在模型训练数据上得分挂零，这种鲜明的对比揭示了其策略性的不透明。

随着全球政策制定者（如欧盟AI法案）开始强制要求某些类型的透明度，这份报告不仅是对现状的记录，更是对未来政策干预方向的指引。如果市场竞争无法带来透明，那么更激进的政策干预或许将成为必然。