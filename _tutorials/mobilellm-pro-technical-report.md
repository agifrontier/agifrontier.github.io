---
layout: default
title: "MobileLLM-Pro Technical Report"
---

## MobileLLM-Pro Technical Report

- **ArXiv URL**: http://arxiv.org/abs/2511.06719v1

### 引言

在人工智能领域，一边是动辄千亿万亿参数的巨无霸模型，它们能力强大，但需要巨大的计算资源。另一边，则是专为手机、笔记本、可穿戴设备等终端设计的**轻量级模型**。

这些模型参数量通常在20亿以下，目标是在不联网的情况下，也能提供低延迟、保护隐私的AI功能。

然而，要让这些小模型既聪明能干，又能处理长篇大论的文本，还要能轻松部署，这是一大挑战。

本文介绍的 **MobileLLM-Pro** 就是一个专为端侧部署优化的10亿参数语言模型。它通过一系列创新，在11个标准基准测试中，性能显著超越了同级别的 Gemma 3-1B 和 Llama 3.2-1B 模型，同时支持高达128k的上下文窗口，并且在4-bit量化后性能几乎没有衰减。

### 模型架构

MobileLLM-Pro 基于标准的 Transformer 框架，并借鉴了 Llama 系列的设计。尽管只有10.8亿参数，但它采用了与 Llama4 等前沿模型相同的 **202,048** 大词汇表。

为了节省参数，它采用了**嵌入层共享**（embedding sharing）技术，即输入嵌入层和输出的语言模型头共享同一套参数，这一下就减少了约2.6亿参数。

模型具体规格如下：

- **层数**：30层
- **注意力头**：20个
- **KV头**：4个（使用了分组查询注意力 GQA）
- **隐藏层维度**：1280
- **总参数量**：10.8亿

为了支持 **128,000 Token** 的超长上下文，模型采用了**局部-全局注意力**（Local-Global Attention）机制。具体来说，每3个只关注最近512个Token的局部注意力层之后，会插入1个能看到全文的全局注意力层。这种设计在长文本处理能力和计算效率之间取得了很好的平衡。

### 多阶段预训练策略

模型的训练过程被精心设计为四个阶段，每个阶段都有明确的目标。

<img src="/images/2511.06719v1/process.jpg" alt="预训练流程图" style="width:90%; max-width:700px; margin:auto; display:block;">

在所有预训练阶段，模型都使用了**知识蒸馏** (Knowledge Distillation, KD) 技术。简单说，就是让一个更强大的教师模型（Llama 4-Scout）先“思考”一遍，然后把它的“思考过程”（logits）教给学生模型 MobileLLM-Pro，而不是只告诉学生标准答案。这能让学生学得更快更好。

### 阶段一：语言能力获取

这是训练的第一步，也是最长的一步，模型在此阶段学习了 **1.4万亿** Token 的数据。

为了解决小模型对数据质量敏感的问题，本文采用了一种名为**可扩展数据混合器**（Scalable Data Mixer, SDM）的策略。它通过模拟训练来自动确定不同领域数据（如通用文本、知识、推理、代码）的最佳混合比例，从而优化初始模型的语言基础。

### 阶段二：使用隐式位置蒸馏扩展上下文

要让模型处理长文本，通常需要用长文本数据进行专门训练。但这种做法会带来一个问题：从短文本训练切换到长文本训练，数据分布变化太大，模型容易“忘记”之前学到的知识。

为了解决这个问题，本文提出了一种名为**隐式位置蒸馏**（Implicit Positional Distillation）的创新方法。

<img src="/images/2511.06719v1/impl_pos_emb_2.jpg" alt="隐式位置蒸馏示意图" style="width:90%; max-width:700px; margin:auto; display:block;">

这个方法很巧妙，可以把它想象成教一个学生读一幅长长的卷轴。

传统的长下文训练，就像是把整幅卷轴铺开，让学生从头看到尾。这不仅费力，学生还可能看到后面忘了前面。

而**隐式位置蒸馏**换了一种教法：老师不给学生看完整的长卷轴。而是从卷轴上相距很远的地方，分别撕下两段**短内容**，然后告诉学生：“这两段内容，在原文里其实隔了很远”。

学生虽然只看到了短内容，但通过老师的指点，它学会了理解和关联**远距离位置信息**的能力。这样，它就“隐式”地掌握了处理长卷轴的本领，而无需真的去处理消耗巨大的长卷轴数据。

通过这种方式，MobileLLM-Pro 在不接触实际长文本数据的情况下，成功将上下文能力扩展到了128k，同时避免了知识遗忘。

### 阶段三：专家模型合并

在训练的后期，为了进一步提升模型的综合能力，本文又引入了一个新方法：**专家模型合并**（Specialist Model Merging）。

这个方法的思路是，与其让一个模型成为“全才”，不如培养一队“专家”。

<img src="/images/2511.06719v1/btm.jpg" alt="专家模型合并示意图" style="width:85%; max-width:600px; margin:auto; display:block;">

具体做法是，在第二阶段训练完成后，将模型克隆出好几份，分别在不同领域的高质量数据上继续训练，比如一份专门学**编程**，一份专门学**推理**。这样就得到了一系列各有所长的“专家模型”。

最后，不是简单地把它们的能力平均一下，而是通过**非均匀权重平均**的方式，将这些专家的“大脑”（模型权重）融合在一起，形成一个统一的、更强大的模型。公式如下：


{% raw %}$$
M = \frac{1}{n} \sum_{b=1}^{n} \theta_{b} * w_{b}
$${% endraw %}


令人惊讶的是，融合后的模型在大多数任务上的表现，甚至超过了任何一个单一的专家。

### 阶段四：量化感知训练

为了让模型能在手机等设备上高效运行，必须对它进行“瘦身”，也就是**量化**（Quantization）。

但直接对训练好的模型进行量化（称为 Post-Training Quantization, PTQ），会导致性能严重下降。

因此，本文采用了**量化感知训练**（Quantization-Aware Training, QAT）。简单说，就是在训练的最后阶段，把量化操作本身也加进去。模型在训练时就知道自己将来要被“压缩”，从而提前学会适应，以保持性能。

本文针对 **CPU** 和**专用加速器**（如 Apple ANE、高通 HTP）设计了两种不同的4-bit量化方案，确保了模型在不同硬件上的高效运行。

实验结果表明，经过 QAT 训练后，量化对模型性能的影响非常小，平均分仅下降了0.7到1.3个百分点。

### 指令微调与效果评估

在强大的预训练基础模型之上，本文通过三阶段的**指令微调**（Instruction Fine-Tuning），打造了一个能够进行聊天、问答、代码生成、函数调用等任务的智能体助手。

1.  **多样性优先**：第一阶段，使用大量多样化的指令数据，建立广泛的能力基础。
2.  **“留一法”调优**：第二阶段，通过“留一法”（Leave-One-Out）实验，评估每个数据集对模型能力的影响，据此调整数据配比，进行针对性强化。
3.  **安全对齐**：最后阶段，通过监督微调（SFT）和直接偏好优化（DPO），提升模型的安全性。

<img src="/images/2511.06719v1/radar_loo.jpg" alt="指令微调结果" style="width:85%; max-width:600px; margin:auto; display:block;">

在最终的评测中，MobileLLM-Pro 的指令微调版本在编码、问答、函数调用等七个基准测试中，全面超越了 Gemma 3-1B 和 Llama 3.2-1B。这证明了其作为端侧智能体助手的强大实力。

### 总结

MobileLLM-Pro 通过四大核心创新，成功打造了一款性能卓越的10亿参数端侧模型：

1.  **模拟驱动的数据混合**，解决了小模型对数据敏感的问题。
2.  **隐式位置蒸馏**，在不使用长文本数据的情况下，高效地赋予了模型长上下文能力。
3.  **专家模型合并**，通过融合多个领域专家的权重，在不增加参数量的前提下提升了模型综合性能。
4.  **量化感知训练**，确保了模型在4-bit量化后依然保持强大的推理和长上下文能力。

这些方法的结合，使得 MobileLLM-Pro 在同级别模型中脱颖而出，为在手机等资源受限设备上实现强大、普惠的AI应用铺平了道路。