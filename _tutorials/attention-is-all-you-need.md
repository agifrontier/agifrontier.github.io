---
layout: default
title: Attention Is All You Need
---

# TL;DR
本文提出了一种名为 Transformer 的全新网络架构，它完全摒弃了循环（Recurrence）和卷积（Convolution）结构，仅依赖注意力机制（Attention Mechanism）来处理序列到序列（sequence-to-sequence）的任务，从而实现了更高的并行度、更少的训练时间，并在机器翻译任务上取得了业界顶尖（SOTA）的成果。

# 关键定义
*   **Transformer**：本文提出的核心模型架构。它遵循一个编码器-解码器（Encoder-Decoder）结构，但其内部完全由堆叠的自注意力层和逐位置的前馈网络构成，不使用任何循环神经网络（RNN）或卷积神经网络（CNN）。
*   **自注意力机制 (Self-Attention)**：也称为内部注意力（intra-attention），是一种将单个序列中的不同位置关联起来以计算该序列的表示的注意力机制。在 Transformer 中，它是捕捉序列内部长距离依赖关系的核心模块。
*   **缩放点积注意力 (Scaled Dot-Product Attention)**：本文所使用的特定注意力实现方式。其输入包含查询（Query, Q）、键（Key, K）和值（Value, V）。通过计算 Q 和 K 的点积，除以一个缩放因子 `$\sqrt{d\_k}$`（`$d\_k$` 是键的维度），再经过 Softmax 函数得到 V 的权重，最后进行加权求和。该缩放因子旨在防止因点积过大导致梯度消失。
    

    $$ \mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^{T}}{\sqrt{d_{k}}})V $$


*   **多头注意力 (Multi-Head Attention)**：一种注意力机制的扩展。它并非执行单次注意力计算，而是将 Q、K、V 通过不同的线性变换投影多次（称为“头”），并行地执行多次缩放点积注意力计算，然后将所有头的输出拼接并再次进行线性变换。这使得模型能同时关注来自不同位置、不同表示子空间的信息。
*   **位置编码 (Positional Encoding)**：由于 Transformer 模型不包含循环或卷积，它本身无法感知序列中词的顺序。位置编码是一种向模型输入中注入词符（token）绝对或相对位置信息的技术。本文使用不同频率的正弦和余弦函数来生成位置编码，并将其与词嵌入相加。
    

    $$ PE_{(pos,2i)}=sin(pos/10000^{2i/d_{\text{model}}}) $$


    

    $$ PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{\text{model}}}) $$



# 相关工作
在本文提出之前，主流的序列转导任务（如机器翻译）通常由包含编码器和解码器的复杂循环神经网络（如 LSTM、GRU）主导。这些模型通过注意力机制连接编码器和解码器，性能优越。

然而，这些模型存在一个关键瓶颈：**循环的本质是顺序计算**。RNN 按照输入序列的时间步 `t` 依次生成隐藏状态 `h_t` ， `h_t` 依赖于前一个状态 `h_{t-1}`。这种固有的顺序性严重阻碍了训练过程中的并行化，尤其是在处理长序列时。虽然一些工作尝试使用卷积网络（如 ConvS2S）来减少顺序依赖，但它们需要通过堆叠多层才能捕捉长距离依赖关系，导致信号传播的路径长度随距离增长。

因此，本文旨在解决的核心问题是：**如何构建一个强大的序列转导模型，既能有效捕捉长距离依赖，又能完全摆脱顺序计算的束缚，从而实现大规模并行计算，提高训练效率和模型性能。**

# 本文方法
## 整体架构
本文提出的 Transformer 模型遵循经典的编码器-解码器架构。编码器将输入的符号序列 `$(x\_1, ..., x\_n)$` 映射为连续表示序列 `$\mathbf{z}=(z\_1, ..., z\_n)$`。解码器根据 `$\mathbf{z}$` 生成输出序列 `$(y\_1, ..., y\_m)$`。与以往模型不同的是，Transformer 的编码器和解码器均未使用循环或卷积层，而是完全基于堆叠的自注意力和逐位置的全连接前馈网络。

<img src="/images/1706.03762v7/ModalNet-21.jpg" alt="The Transformer - model architecture." style="width:80%; max-width:400px; margin:auto; display:block;">

### 编码器与解码器栈
*   **编码器 (Encoder)**：由 N=6 个相同的层堆叠而成。每一层包含两个子层：
    1.  一个多头自注意力机制（Multi-Head Self-Attention）。
    2.  一个简单的、逐位置的全连接前馈网络（Position-wise Fully Connected Feed-Forward Network）。
    每个子层的输出都采用了残差连接（Residual Connection）和层归一化（Layer Normalization），即 `LayerNorm(x + Sublayer(x))`。模型中所有子层和嵌入层的输出维度均为 `$d\_{\text{model}}=512$`。

*   **解码器 (Decoder)**：同样由 N=6 个相同的层堆叠而成。每一层包含三个子层：
    1.  一个**带掩码的**多头自注意力机制（Masked Multi-Head Self-Attention），确保在预测位置 `i` 时只能依赖于 `i` 之前的已知输出，保持自回归（auto-regressive）特性。
    2.  一个多头注意力机制，其查询（Query）来自前一解码器子层，而键（Key）和值（Value）来自编码器的输出。这使得解码器中的每个位置都能关注到输入序列的所有位置。
    3.  一个逐位置的全连接前馈网络。
    解码器同样在每个子层周围使用残差连接和层归一化。

### 注意力机制
<img src="/images/1706.03762v7/ModalNet-20.jpg" alt="（左）缩放点积注意力。（右）多头注意力由多个并行的注意力层组成。" style="width:80%; max-width:400px; margin:auto; display:block;">
Transformer模型中的注意力机制是其核心创新，具体分为两种形式：

*   **缩放点积注意力 (Scaled Dot-Product Attention)**：这是注意力的基本单元。其计算公式为：
    

    $$ \mathrm{Attention}(Q,K,V)=\mathrm{softmax}(\frac{QK^{T}}{\sqrt{d\_{k}}})V $$


    其中，`$Q, K, V$` 分别代表查询、键和值矩阵。除以 `$\sqrt{d\_k}$` 这个缩放因子是为了防止当 `d_k` 较大时，点积结果过大，将 softmax 函数推入梯度极小的区域，从而稳定训练过程。

*   **多头注意力 (Multi-Head Attention)**：为了让模型能够从不同角度关注信息，本文没有使用单一的注意力函数，而是将查询、键和值分别通过不同的线性变换投影 `$h$` 次（`$h=8$`），然后并行地执行 `$h$` 个缩放点积注意力计算。每个“头”的输出维度被缩小为 `$d\_{\text{model}}/h = 64$`。最后，将所有头的输出结果拼接起来，再通过一次线性变换得到最终输出。
    

    $$ \mathrm{MultiHead}(Q,K,V) = \mathrm{Concat}(\mathrm{head_1},...,\mathrm{head_h})W^{O} $$


    

    $$ \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i}) $$


    多头机制允许模型共同关注来自不同表示子空间的信息，这是单一注意力头难以实现的。

### 为什么是自注意力
本文详细对比了自注意力层与循环层、卷积层的优劣，总结在下表中。

| 层类型 | 每层计算复杂度 | 顺序操作数 | 最大路径长度 |
| --- | --- | --- | --- |
| 自注意力 | `$O(n^{2}\cdot d)$` | `$O(1)$` | `$O(1)$` |
| 循环 | `$O(n\cdot d^{2})$` | `$O(n)$` | `$O(n)$` |
| 卷积 | `$O(k\cdot n\cdot d^{2})$` | `$O(1)$` | `$O(\log\_{k}(n))$` |
| 受限自注意力 | `$O(r\cdot n\cdot d)$` | `$O(1)$` | `$O(n/r)$` |

自注意力的核心优势在于：
1.  **并行性**：它在层内的计算复杂度仅为 `$O(1)$` 级别的顺序操作，可以大规模并行计算，不像 RNN 需要 `$O(n)$` 的顺序操作。
2.  **长距离依赖**：它能以常数级别的路径长度连接任意两个位置，这使得学习长距离依赖关系变得更加容易，而 RNN 和 CNN 的路径长度会随序列长度 `$n$` 或卷积核大小 `$k$` 改变。
3.  **可解释性**：自注意力机制产生的注意力分布可以帮助我们理解模型在做决策时关注了输入的哪些部分。

### 其他组件
*   **逐位置前馈网络 (Position-wise Feed-Forward Networks)**：每个注意力子层之后都接一个前馈网络，它由两个线性变换和一个 ReLU 激活函数组成，对序列中的每个位置独立且相同地进行变换。
    

    $$ \mathrm{FFN}(x)=\max(0,xW\_{1}+b\_{1})W\_{2}+b\_{2} $$


*   **位置编码 (Positional Encoding)**：由于模型结构不包含序列顺序信息，本文通过在输入嵌入中加入位置编码来注入位置信息。该编码使用正弦和余弦函数生成，并且其维度与词嵌入相同（`$d\_{\text{model}}$`），因此可以直接相加。

# 实验结论
本文在机器翻译和英语成分句法分析两个任务上对 Transformer 模型进行了评估。

### 机器翻译
在 WMT 2014 英语-德语和英语-法语翻译任务上，Transformer 取得了卓越的成绩。

| 模型 | BLEU (EN-DE) | BLEU (EN-FR) | 训练成本 (FLOPs) |
| --- | :---: | :---: | :---: |
| GNMT + RL [38] | 24.6 | 39.92 | `$2.3\cdot 10^{19}$` |
| ConvS2S [9] | 25.16 | 40.46 | `$9.6\cdot 10^{18}$` |
| Transformer (base model) | **27.3** | 38.1 | **`$3.3\cdot 10^{18}$`** |
| Transformer (big) | **28.4** | **41.8** | `$2.3\cdot 10^{19}$` |

*   **性能**：在英-德翻译任务上，**Transformer (big)** 模型取得了 28.4 的 BLEU 分数，比之前所有已发表的模型（包括集成模型）高出 2.0 以上，创造了新的 SOTA。在英-法任务上，其 BLEU 分数达到 41.8，同样是新的单模型 SOTA。
*   **效率**：Transformer 的训练成本远低于其他竞争模型。例如，基础模型（base model）在超越所有先前模型的同时，训练成本仅为它们的很小一部分。大模型（big model）在 8 个 P100 GPU 上仅训练 3.5 天就达到了 SOTA 水平。

### 模型变体分析
通过消融实验，本文验证了 Transformer 各个组件的重要性。

| 变体 | 描述 | BLEU (dev) | 结论 |
| :---: | --- | :---: | --- |
| A | 调整注意力头数 `h` | 24.9 ~ 25.8 | `h=8`时最佳。单头（`h=1`）效果差，头数过多（`h=32`）也会导致性能下降。 |
| B | 减小键的维度 `d_k` | 25.1 | 减小 `d_k` 会损害模型质量，表明点积之外更复杂的兼容性函数可能有用。 |
| C & D | 改变模型大小和正则化 | 26.2 / 25.7 | 更大的模型效果更好，Dropout 和标签平滑对于防止过拟合非常关键。 |
| E | 使用可学习的位置嵌入 | 25.7 | 与固定的正弦位置编码效果几乎相同。 |

### 英语成分句法分析
为了测试模型的泛化能力，本文将其应用于英语成分句法分析。

| Parser | 训练数据 | WSJ 23 F1 |
| --- | --- | :---: |
| Dyer et al. (2016) [8] | WSJ only | 91.7 |
| **Transformer (4 layers)** | **WSJ only** | **91.3** |
| Vinyals & Kaiser el al. (2014) [37] | semi-supervised | 92.1 |
| **Transformer (4 layers)** | **semi-supervised** | **92.7** |

*   **泛化性**：尽管没有进行大量任务相关的调优，Transformer 表现出惊人的性能，其结果优于除循环神经网络语法（Recurrent Neural Network Grammar）之外的所有先前模型。特别地，在仅使用少量数据（4万个句子）训练时，Transformer 的表现也超过了之前的强基线模型。

### 总结
本文提出的 Transformer 模型是第一个完全基于注意力机制的序列转导模型。它在翻译任务上不仅训练速度显著快于基于循环或卷积的架构，还创造了新的 SOTA 记录。实验证明，该模型具有优异的性能、高效率和良好的泛化能力，为未来的序列建模研究开辟了新的方向。