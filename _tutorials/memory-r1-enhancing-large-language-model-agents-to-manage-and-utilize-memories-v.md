---
layout: default
title: "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning"
---

# Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning

- **ArXiv URL**: http://arxiv.org/abs/2508.19828v1

- **作者**: Ercong Nie; Yunpu Ma; Zifeng Ding; Xiaowen Ma; Hinrich Schütze; Volker Tresp

- **发布机构**: Ludwig Maximilian University of Munich; Technical University of Munich; University of Cambridge; University of Hong Kong

---

# TL;DR
本文提出 Memory-R1 框架，通过强化学习（Reinforcement Learning, RL）训练一个专门用于执行结构化内存操作的$$内存管理器$$智能体和一个用于筛选并推理记忆的$$问答智能体$$，从而让大型语言模型（LLM）能够主动、高效地管理和利用外部记忆，以解决长时程推理任务。

# 关键定义
*   **Memory-R1**: 一个基于强化学习的框架，旨在增强 LLM 智能体的记忆能力。它由两个核心智能体组成：内存管理器和问答智能体，二者通过结果驱动的奖励信号进行微调，以学习如何管理和利用记忆。
*   **内存管理器 (Memory Manager)**: Memory-R1 中的一个专门智能体，负责维护外部记忆库。它学习选择四种结构化操作之一：$$ADD$$ (新增)、$$UPDATE$$ (更新)、$$DELETE$$ (删除) 或 $$NOOP$$ (无操作)，以动态、准确地调整记忆内容。
*   **问答智能体 (Answer Agent)**: Memory-R1 中的另一个专门智能体，负责利用记忆库回答问题。它首先通过“内存蒸馏”策略从检索到的众多记忆中筛选出最相关的条目，然后基于这些精炼后的信息进行推理并生成最终答案。
*   **内存蒸馏 (Memory Distillation)**: 问答智能体采用的一项策略。面对通过 RAG 检索到的大量（例如60个）候选记忆，该策略会过滤掉不相关或有干扰的条目，只保留对回答当前问题最关键的核心记忆，从而提高推理的准确性和效率。

# 相关工作
当前，大型语言模型（LLM）因其有限的上下文窗口而具有“无状态”的根本缺陷，这限制了它们在长对话或多阶段任务中进行长时程推理的能力。

为了解决这一瓶颈，主流研究方向是为 LLM 配备外部记忆模块，通常采用检索增强生成（Retrieval-Augmented Generation, RAG）的范式。然而，现有方法存在两大关键问题：
1.  **静态和启发式驱动**: 记忆管理（如何新增、更新或删除信息）大多依赖于基于规则的启发式方法或未经专门训练的 LLM，这使得系统在处理复杂或矛盾信息时容易出错（例如，将信息更新误判为矛盾并错误删除旧信息）。
2.  **检索噪音**: RAG 系统检索到的信息良莠不齐，可能包含大量无关内容。模型被迫在混合了相关和无关信息的上下文中进行推理，容易受到噪音干扰，影响最终答案的质量。

本文旨在解决上述问题，即如何让 LLM 智能体学会**自适应地管理记忆**和**有效地利用检索到的记忆**。本文认为强化学习是解决此问题的关键，通过结果驱动的奖励来训练智能体做出最优的记忆操作和使用决策，而不是依赖于脆弱的启发式规则。

# 本文方法
本文提出了 Memory-R1 框架，通过强化学习分别对“内存管理器”和“问答智能体”进行微调，以实现对长期记忆的智能管理和利用。

![](CameraReady/LaTeX/figure2_pipeline.png)
**图2**: Memory-R1 框架概览。阶段1（蓝色）通过 RL 微调的内存管理器构建和更新记忆库，为每个对话轮次选择 {ADD, UPDATE, DELETE, NOOP} 操作。阶段2（绿色）通过问答智能体回答用户问题，该智能体应用内存蒸馏策略对检索到的记忆进行推理。

### 总体框架
Memory-R1 的流程分为两个主要阶段：

1.  **记忆库构建与维护**：在每个对话轮次中，系统首先从当前对话中提取关键信息。随后，利用该信息通过 RAG 在现有记忆库中检索相关条目。最后，$$内存管理器$$智能体接收新信息和检索到的旧记忆，并决定执行 $$ADD$$、$$UPDATE$$、$$DELETE$$ 或 $$NOOP$$ 中的哪一个操作来更新记忆库。

2.  **问答**：当用户提出问题时，系统首先通过 RAG 从记忆库中检索多达60个候选记忆。然后，$$问答智能体$$应用其“内存蒸馏”策略，从这些候选项中筛选出最相关的记忆。最后，智能体基于这些被提纯的记忆进行推理，生成最终答案。

### 内存管理器智能体的强化学习微调
**任务与创新点**：内存管理器的任务是学会如何动态、准确地维护记忆库。其创新之处在于**不依赖人工标注或启发式规则**，而是通过 RL 从任务最终结果中学习最优的记忆操作策略。

*   **策略模型**: 内存管理器的策略 $\pi\_{\theta}$ 将新信息 $x$ 和相关的旧记忆 $\mathcal{M}\_{\text{old}}$ 作为输入，输出一个操作 $o$ 和更新后的内容 $m^{\prime}$：
    

    {% raw %}$$
    (o, m^{\prime}) \sim \pi_{\theta}(\cdot \mid x, \mathcal{M}_{\text{old}})
    $${% endraw %}


*   **RL 算法**: 本文同时使用了 PPO 和 GRPO 算法进行微调。
    *   **PPO**: 优化一个裁剪的替代目标函数，以稳定训练过程。
        

        {% raw %}$$
        \mathcal{J}(\theta)=\mathbb{E}\left[\min\left(\rho_{\theta}A,\;\text{clip}(\rho_{\theta},1-\epsilon,1+\epsilon)A\right)\right]
        $${% endraw %}


        其中 $\rho\_{\theta}$ 是新旧策略的重要性比率， $A$ 是优势函数。
    *   **GRPO**: 通过对一组候选动作的奖励进行归一化来计算相对优势，从而无需价值函数。
        

        {% raw %}$$
        \mathcal{J}(\theta)=\mathbb{E}\Bigg{[}\frac{1}{G}\sum_{i=1}^{G}\rho_{\theta}^{(i)}A_{i}-\beta\,\mathbb{D}_{\text{KL}}\big{[}\pi_{\theta}\,\ \mid \,\pi_{\text{ref}}\big{]}\Bigg{]}
        $${% endraw %}


*   **奖励设计**: 采用**结果驱动 (outcome-driven)** 的奖励机制。内存管理器的操作是否有效，取决于更新后的记忆库能否帮助（一个固定的）问答智能体正确回答问题。奖励直接定义为最终答案的准确性（Exact Match, EM）：
    

    {% raw %}$$
    R_{answer} = \mathrm{EM}(y_{\text{pred}}, y_{\text{gold}})
    $${% endraw %}


    这种设计大大简化了监督过程，使其具有可扩展性。

### 问答智能体的强化学习微调
**任务与创新点**：问答智能体旨在学习如何从嘈杂的检索结果中筛选出真正有用的信息（即内存蒸馏），并基于此进行精确推理。其创新之处在于将**记忆的筛选和利用过程也纳入了 RL 的学习范畴**。

*   **策略模型**: 问答智能体的策略 $\pi\_{\text{ans}}$ 将问题 $q$ 和检索到的记忆 $\mathcal{M}\_{\text{ret}}$作为输入，生成答案 $y$：
    

    {% raw %}$$
    y \sim \pi_{\text{ans}}(\cdot \mid q, \mathcal{M}_{\text{ret}})
    $${% endraw %}


*   **RL 算法**: 同样使用 PPO 和 GRPO 进行微调，优化过程与内存管理器类似，但动作空间是生成答案的 Token 序列。
*   **奖励设计**: 奖励函数同样简单直接，即生成答案与标准答案之间的精确匹配（EM）分数：
    

    {% raw %}$$
    R_{answer} = \mathrm{EM}(y_{\text{pred}}, y_{\text{gold}})
    $${% endraw %}



# 实验结论
本文在 LOCOMO 基准数据集上进行了广泛的实验，该数据集专门用于测试模型在长程、多会话对话中的记忆和推理能力。

### 主要结果
Memory-R1 在两个不同的大模型基座（LLaMA‑3.1‑8B‑Instruct 和 Qwen‑2.5‑7B‑Instruct）上均取得了当前最佳性能，显著优于包括 Mem0 在内的所有基线模型。

*   **性能提升**: 在 LLaMA-3.1-8B 上，与最强的基线 Mem0 相比，Memory-R1-GRPO 在 F1、BLEU-1 和 LLM-as-a-Judge 三个指标上分别提升了 **48%**、**69%** 和 **37%**。
*   **模型泛化性**: 该方法在 Qwen-2.5-7B 基座上也表现出类似的巨大优势，证明了 Memory-R1 框架的有效性与模型架构无关。
*   **数据高效性**: 值得注意的是，这些显著的性能提升仅用了 **152 个问答对**和相应的记忆库进行训练，证明了该方法的学习效率非常高。


| 模型 | 方法 | 单跳 | | | 多跳 | | | 开放域 | | | 时序 | | | 总体 | | |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| | | F1$\uparrow$ | B1$\uparrow$ | J$\uparrow$ | F1$\uparrow$ | B1$\uparrow$ | J$\uparrow$ | F1$\uparrow$ | B1$\uparrow$ | J$\uparrow$ | F1$\uparrow$ | B1$\uparrow$ | J$\uparrow$ | F1$\uparrow$ | B1$\uparrow$ | J$\uparrow$ |
| LLaMA-3.1-8B Instruct | LOCOMO | 12.25 | 9.77 | 13.81 | 13.69 | 10.96 | 20.48 | 11.59 | 8.30 | 15.96 | 9.38 | 8.15 | 4.65 | 11.41 | 8.71 | 13.62 |
| | Zep | 30.15 | 17.15 | 52.38 | 15.04 | 11.56 | 33.33 | 26.67 | 18.44 | 45.36 | 3.49 | 2.68 | 27.58 | 22.60 | 15.05 | 42.80 |
| | A-Mem | 21.62 | 16.93 | 44.76 | 13.82 | 11.45 | 34.93 | 34.67 | 29.13 | 49.38 | 25.77 | 22.14 | 36.43 | 29.20 | 24.40 | 44.76 |
| | LangMem | 22.40 | 15.21 | 47.26 | 18.65 | 16.03 | 39.81 | 31.62 | 23.85 | 48.38 | 27.75 | 21.53 | 30.94 | 28.34 | 21.31 | 44.18 |
| | Mem0 | 27.29 | 18.63 | 43.93 | 18.59 | 13.86 | 37.35 | 34.03 | 24.77 | 52.27 | 26.90 | 21.06 | 31.40 | 30.41 | 22.22 | 45.68 |
| | Memory-R1-PPO | 32.52 | 24.47 | 53.56 | 26.86 | 23.47 | 42.17 | 45.30 | 39.18 | 64.10 | 41.57 | 26.11 | 47.67 | 41.05 | 32.91 | 57.54 |
| | **Memory-R1-GRPO** | **35.73** | **27.70** | **59.83** | **35.65** | **30.77** | **53.01** | **47.42** | **41.24** | **68.78** | **49.86** | **38.27** | **51.55** | **45.02** | **37.51** | **62.74** |
| Qwen-2.5-7B Instruct | LOCOMO | 9.57 | 7.00 | 15.06 | 11.84 | 10.02 | 19.28 | 8.67 | 6.52 | 12.79 | 8.35 | 8.74 | 5.43 | 8.97 | 7.27 | 12.17 |
| | Zep | 31.02 | 21.39 | 42.85 | 20.42 | 15.76 | 23.81 | 25.25 | 21.34 | 42.26 | 8.94 | 8.42 | 29.31 | 22.32 | 18.78 | 38.99 |
| | A-Mem | 18.96 | 12.86 | 40.78 | 14.73 | 12.66 | 31.32 | 30.58 | 26.14 | 46.90 | 23.67 | 20.67 | 28.68 | 26.08 | 21.78 | 40.78 |
| | LangMem | 22.84 | 16.98 | 43.64 | 18.98 | 16.89 | 44.38 | 32.47 | 25.98 | 50.45 | 26.62 | 20.93 | 23.08 | 28.69 | 22.76 | 43.42 |
| | Mem0 | 24.96 | 18.05 | 61.92 | 20.31 | 15.82 | 48.19 | 32.74 | 25.27 | 65.20 | 33.16 | 26.28 | 38.76 | 30.61 | 23.55 | 53.30 |
| | Memory-R1-PPO | 34.22 | 23.61 | 57.74 | 32.87 | 29.48 | 53.01 | 44.78 | 38.72 | 66.99 | 42.88 | 30.30 | 42.25 | 41.72 | 33.70 | 59.53 |
| | **Memory-R1-GRPO** | **33.64** | **26.06** | **62.34** | **23.55** | **20.71** | **40.96** | **46.86** | **40.92** | **67.81** | **47.75** | **38.49** | **49.61** | **43.14** | **36.44** | **61.51** |
### 消融实验


| 方法 | F1$\uparrow$ | B1$\uparrow$ | J$\uparrow$ |
| --- | --- | --- | --- |
| LLaMA3.1-8B (基线) | 26.73 | 20.54 | 47.82 |
| LLaMA3.1-8B + PPO | 32.55 | 24.60 | 59.37 |
| LLaMA3.1-8B + GRPO | **33.05** | **24.91** | **59.91** |
**表2**: RL微调的内存管理器性能。结果表明，与基线（仅靠上下文指令）相比，经过PPO和GRPO微调的内存管理器性能显著提升。


| 方法 | F1$\uparrow$ | B1$\uparrow$ | J$\uparrow$ |
| --- | --- | --- | --- |
| LLaMA3.1-8B (基线) | 26.73 | 20.54 | 47.82 |
| LLaMA3.1-8B + PPO | 34.48 | 28.13 | 49.04 |
| LLaMA3.1-8B + GRPO | **37.54** | **30.64** | **52.87** |
**表3**: RL微调的问答智能体性能。结果显示，经过RL微调的问答智能体在所有指标上都优于基线模型。


| 方法 | F1$\uparrow$ | B1$\uparrow$ | J$\uparrow$ |
| --- | --- | --- | --- |
| GRPO 无内存蒸馏 | 40.95 | 34.37 | 60.14 |
| GRPO 有内存蒸馏 | **45.02** | **37.51** | **62.74** |
**表4**: 内存蒸馏的效果。结果证明，内存蒸馏策略能有效过滤噪音，显著提升问答智能体的性能。

![](CameraReady/LaTeX/answer_agent_comparison.png)
**图3**: 不同问答智能体与不同内存管理器组合的性能。结果显示，RL微调的问答智能体（PPO/GRPO）在与**更强大**的内存管理器（GPT-4o-mini）配对时，性能增益更大，证明了该方法具有良好的扩展性和协同效应。

![](CameraReady/LaTeX/RL_training_comparison.png)
**图4**: PPO与GRPO的训练奖励曲线。GRPO在训练初期收敛更快，但两种方法最终都能达到相似的高奖励水平。

### 总结
实验结果有力地证明，Memory-R1 框架通过强化学习，成功地教会了 LLM 智能体如何主动并准确地管理和利用记忆。其核心贡献在于验证了**用结果驱动的 RL 替代静态启发式规则**是构建自适应、记忆增强型智能体的有效路径。该方法不仅性能卓越，而且具有高数据效率和良好的模型泛化性，为开发更具智能体行为、能够进行长期知识积累和推理的 LLM 系统开辟了新的方向。