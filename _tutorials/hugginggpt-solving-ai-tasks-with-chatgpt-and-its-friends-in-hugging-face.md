---
layout: default
title: "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"
---

# HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face

- **ArXiv URL**: http://arxiv.org/abs/2303.17580v4

- **作者**: Yongliang Shen; Xu Tan; Dongsheng Li; Y. Zhuang; Weiming Lu; Kaitao Song

- **发布机构**: Microsoft Research Asia; Zhejiang University

---

# TL;DR
本文提出了一个名为 HuggingGPT 的 AI 智能体系统，它以大型语言模型 (LLM) 为控制器，利用自然语言作为通用接口，自动地规划任务、选择并执行 Hugging Face 社区中的各种 AI 模型，以解决复杂的多模态 AI 任务。

# 关键定义
*   **HuggingGPT**: 一个由大型语言模型（LLM）驱动的智能体 (agent)，能够连接 LLMs（如 ChatGPT）和机器学习社区（如 Hugging Face）中的海量AI模型，以自主解决复杂的 AI 任务。它将 LLM 作为任务规划和决策的“大脑”，将其他专家模型作为执行具体任务的“执行器”。

*   **将语言作为通用接口 (Language as a generic interface)**: 这是 HuggingGPT 的核心理念。它指通过自然语言描述来概括每个 AI 模型的功能，LLM 可以理解这些描述，从而能够像调用函数一样选择、组合和调度这些外部模型来协同工作。

*   **任务规划 (Task Planning)**: HuggingGPT 工作流的第一阶段。LLM 负责分析用户的自然语言请求，理解其意图，并将其分解成一个结构化的、可执行的任务列表，同时确定任务之间的依赖关系和执行顺序。

*   **模型选择 (Model Selection)**: 工作流的第二阶段。根据任务规划阶段生成的任务列表，LLM 负责为每个具体任务从 Hugging Face 的海量模型中选择最合适的一个专家模型，选择的依据是模型的自然语言描述。

# 相关工作
当前，扩展大型语言模型（LLM）能力的研究主要分为两个方向：
1.  **统一多模态模型**：如 Flamingo、BLIP-2 等，旨在构建一个庞大且单一的模型，使其本身具备处理多种模态（文本、图像等）信息的能力。
2.  **LLM + 工具/模型集成**：如 Toolformer、Visual ChatGPT 等，通过让 LLM 学会调用外部 API 或特定模型来扩展其能力范围。

尽管取得了进展，但现有 LLM 仍面临三大瓶颈：
*   **模态限制**：主要处理文本，缺乏处理如视觉、语音等复杂信息的能力。
*   **任务复杂度限制**：难以自主地调度和协同多个模型来完成由多个子任务构成的复杂工作。
*   **专业能力限制**：在许多专业任务上，其零样本（zero-shot）或少样本（few-shot）性能仍不如经过专门微调的专家模型。

本文旨在解决上述问题，通过构建一个框架，让 LLM 充当通用问题解决的“控制器”，有效协调和利用外部专家模型的专业能力，从而处理更广泛、更复杂的跨领域、跨模态 AI 任务。

# 本文方法
HuggingGPT 是一个由 LLM 控制器和众多专家模型执行器组成的协作系统。其工作流如下图所示，分为四个核心阶段，以响应用户请求。

<img src="/images/2303.17580v4/page_2_Figure_0.jpg" alt="HuggingGPT 概览" style="width:85%; max-width:450px; margin:auto; display:block;">

### 任务规划 (Task Planning)
这是系统的第一步。HuggingGPT 利用 LLM 强大的自然语言理解能力，将用户可能复杂的请求分解为一个结构化的任务列表。为了实现高效和准确的规划，本文设计了包含两种核心技术的提示（Prompt）策略：
1.  **基于规范的指令 (Specification-based Instruction)**: 定义了一个标准的 JSON 任务格式，包含四个关键字段：$$task$$（任务类型）、$$id$$（唯一标识）、$$dep$$（依赖任务ID）和 $$args$$（任务所需参数）。LLM 被要求严格遵循此格式输出任务规划结果。
2.  **基于演示的解析 (Demonstration-based Parsing)**: 在提示中加入多个“用户请求-任务规划”的示例（few-shot demonstrations），帮助 LLM 更好地理解任务分解的逻辑、依赖关系的判断以及资源如何在任务间传递。

此外，系统还能通过将聊天记录 $${{ Chat Logs }}$$ 加入提示，来支持多轮对话中的上下文理解。

### 模型选择 (Model Selection)
在任务规划之后，系统需要为每个任务挑选一个最合适的模型。HuggingGPT 将此过程构建为一个基于上下文的单项选择题。为了从 Hugging Face 众多的模型中高效选择，并克服 LLM 的上下文长度限制，该过程分为两步：
1.  **初步筛选**：根据当前任务的类型（如 $$text-to-image$$），从模型库中筛选出一批功能匹配的候选模型。
2.  **精选排序**：将筛选出的模型按其在 Hugging Face 上的下载量进行排序，选择排名最高的 Top-K 个模型。然后将这K个模型的描述信息注入到提示中，让 LLM 从中选出最合适的一个。

这种方法使得系统可以灵活、持续地集成新的专家模型，只需提供其功能描述即可。

### 任务执行 (Task Execution)
此阶段负责执行选定的模型并处理任务间的资源依赖。
*   **资源依赖管理**: HuggingGPT 设计了一个巧妙的机制来解决动态生成的资源传递问题。它使用一个特殊的占位符 $$"<resource>-task_id"$$ 来表示某个任务的输出。在任务规划阶段，如果一个任务依赖于前序任务 $$task_id$$ 的输出，其参数中就会使用这个占位符。在执行阶段，系统会将这个占位符动态地替换为前序任务实际生成的资源路径或数据。

<img src="/images/2303.17580v4/page_1_Figure_0.jpg" alt="语言作为连接LLM与AI模型的接口" style="width:85%; max-width:600px; margin:auto; display:block;">

*   **并行执行**: 对于没有依赖关系的任务，系统会并行执行以提高效率。同时，系统提供了一个混合推理端点，以加速模型推理并保证计算稳定性。

### 响应生成 (Response Generation)
在所有任务执行完毕后，系统进入最后阶段。LLM 会集成前三个阶段的所有信息，包括：
*   用户原始输入
*   规划的任务列表
*   为每个任务选择的模型
*   每个模型的具体推理结果（可能是结构化数据，如目标检测的边界框和置信度）

LLM 最终将这些信息整合，并以流畅、人性化的自然语言生成最终的响应，不仅总结了执行过程和结果，还直接回答了用户的初始问题。

下表展示了 HuggingGPT 中为各个阶段设计的详细提示模板。


| 阶段 | 提示内容 |
| :--- | :--- |
| **#1 任务规划** | AI助手对用户输入进行任务解析，生成一个任务列表，格式为... 任务必须从以下选项中选择：$${{ Available Task List }}$$... 为辅助任务规划，聊天记录可用作 $${{ Chat Logs }}$$... |
| **#2 模型选择** | 给定用户请求和调用命令，AI助手帮助用户从模型列表中选择一个合适的模型... AI助手仅输出最合适模型的ID，输出必须是严格的JSON格式：$${"id": "id", "reason": "你的选择理由"}$$。我们有一个模型列表供你选择 $${{ Candidate Models }}$$。请从列表中选择一个模型。 |
| **#4 响应生成** | 基于输入和推理结果，AI助手需要描述过程和结果。之前的阶段信息如下 - 用户输入: $${{ User Input }}$$, 任务规划: $${{ Tasks }}$$, 模型选择: $${{ Model Assignment }}$$, 任务执行: $${{ Predictions }}$$。你必须首先直接回答用户的请求。然后，以第一人称向用户描述任务过程，并展示你的分析和模型推理结果... |
*Table 1: HuggingGPT 中提示设计的细节。*

# 实验结论
本文通过定量、定性及人工评估，验证了 HuggingGPT 框架的有效性。

**定量评估**
*   **评估核心**：重点评估了不同 LLM（Alpaca-7b, Vicuna-7b, GPT-3.5）在**任务规划**阶段的能力，将其分为三类：**单一任务 (Single Task)**、**顺序任务 (Sequential Task)** 和 **图结构任务 (Graph Task)**。
*   **核心发现**：
    *   结果表明，模型能力越强的 LLM（GPT-3.5），其任务规划的准确率和 F1 值也越高，显著优于 Alpaca 和 Vicuna 等开源模型（见下表）。
    *   在一个高质量的人类标注数据集上，GPT-4 的表现最好，但与人类标注的黄金标准相比仍有较大差距，这凸显了提升 LLM 规划能力的重要性。


| LLM 型号 | 单一任务 (F1 ↑) | 顺序任务 (F1 ↑) | 图任务 (F1 ↑) |
| :--- | :--- | :--- | :--- |
| Alpaca-7b | 4.88 | 22.80 | 20.59 |
| Vicuna-7b | 29.44 | 22.89 | 18.66 |
| GPT-3.5 | 54.45 | 51.92 | 51.91 |
*Tables 3-5: 不同 LLM 在 GPT-4 标注数据集上的任务规划能力对比。*

**消融研究**
*   研究了提示中**演示示例的数量和多样性**对任务规划能力的影响。
*   **发现**：
    *   增加演示示例的**多样性**能适度提升 LLM 的规划性能。
    *   增加演示示例的**数量**在初期有帮助，但当数量超过4个后，性能提升趋于饱和。

<img src="/images/2303.17580v4/page_8_Figure_8.jpg" alt="不同演示数量下任务规划能力的评估" style="width:90%; max-width:700px; margin:auto; display:block;">

**人工评估**
*   对整个系统的端到端表现（任务规划、模型选择、响应生成）进行了人工评估。
*   **发现**：GPT-3.5 在各个阶段的通过率（Passing Rate）和合理性（Rationality）评分上均远超 Alpaca-13b 和 Vicuna-13b，最终的端到端任务**成功率达到了 63.08%**，而 Vicuna-13b 仅为 15.64%。这进一步证明了强大的 LLM 作为控制器在自主智能体框架中的关键作用。


| LLM 型号 | 任务规划 (通过率/合理性) | 模型选择 (通过率/合理性) | 最终成功率 |
| :--- | :--- | :--- | :--- |
| Alpaca-13b | 51.04% / 32.17% | - / - | 6.92% |
| Vicuna-13b | 79.41% / 58.41% | - / - | 15.64% |
| GPT-3.5 | 91.22% / 78.47% | 93.89% / 84.29% | 63.08% |
*Table 8: 不同 LLM 的人工评估结果。*

**局限性**
本文也指出了 HuggingGPT 的一些局限性，包括：
1.  **规划能力的依赖**：系统表现严重依赖 LLM 的规划能力，而 LLM 的规划并非总能保证最优和可行。
2.  **效率问题**：整个工作流需要与 LLM 进行多次交互，导致响应时间较长。
3.  **Token 长度限制**：LLM 有限的上下文窗口限制了能够同时考虑的候选模型的数量。
4.  **不稳定性**：LLM 的输出具有不确定性，可能不遵循指令，导致工作流异常。

**总结**
实验证明，HuggingGPT 框架能够有效理解和解决复杂的多模态 AI 任务。方法的成功与否在很大程度上取决于其核心 LLM 控制器的能力。HuggingGPT 为实现通用人工智能（AGI）提供了一条新的、可扩展的路径。