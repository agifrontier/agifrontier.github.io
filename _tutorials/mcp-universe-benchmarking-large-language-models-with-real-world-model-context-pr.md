---
layout: default
title: "MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers"
---

# TL;DR
本文提出了 MCP-Universe，这是首个旨在通过与真实世界模型上下文协议（Model Context Protocol, MCP）服务器交互来评估大型语言模型（LLM）的综合性基准，实验表明即便是最顶尖的 LLM 在处理长上下文、未知工具和跨领域任务时也面临显著的性能瓶颈。

# 关键定义
*   **模型上下文协议 (Model Context Protocol, MCP)**: 由 Anthropic 提出的一个开放标准，旨在为 AI 系统与外部数据源和工具的集成提供一个统一的接口。它通过标准化的消息传递，解决了以往 AI 系统与外部工具集成时存在的碎片化和“信息孤岛”问题，被称为“AI领域的USB-C”。
*   **MCP-Universe**: 本文提出的一个综合性评测基准，用于评估 LLM 在与真实的 MCP 服务器交互时的能力。它包含 6 个领域、11 个 MCP 服务器和 231 个具有挑战性的现实世界任务。
*   **执行式评估器 (Execution-based Evaluators)**: 一套用于自动、客观地评判任务是否完成的评估工具，取代了依赖 LLM 作为裁判的主观方法。它分为三类：
    *   **格式评估器 (Format Evaluators)**: 检查智能体的输出是否严格遵守预设的格式要求。
    *   **静态评估器 (Static Evaluators)**: 验证任务答案的正确性，适用于答案固定不变的任务（如历史数据）。
    *   **动态评估器 (Dynamic Evaluators)**: 验证需要实时获取信息的任务答案，通过自动获取实时基准真相来评估（如实时股价、天气）。

# 相关工作
当前，针对 LLM 作为智能体的评测基准主要集中在指令遵循、数学推理或孤立的函数调用等方面。虽然近期出现了一些与 MCP 相关的基准（如 MCP-RADAR、MCPWorld），但它们或改编自现有数据集，缺乏真实世界应用的广度；或严重依赖图形用户界面（GUI），未能充分覆盖 MCP 驱动的工作流；或依赖“LLM即裁判”的评估方式，不适用于需要实时数据的动态任务。

因此，现有评测方法无法全面评估 LLM 在真实、复杂的 MCP 环境中的表现，尤其是在处理长序列推理、大规模未知工具集和动态数据等挑战时。

本文旨在解决这一关键空白，通过构建一个基于真实世界 MCP 服务器、包含多样化和高难度任务、并采用严格的执行式评估方法的综合基准，以真实反映当前 LLM 智能体在 MCP 生态中的实际能力和局限性。

# 本文方法

### 框架概览
MCP-Universe 是一个旨在评测 LLM 在真实 MCP 环境下能力的综合框架。其核心由三部分构成：(1) 一个可扩展、易于使用的评估框架；(2) 一系列基于真实世界 MCP 服务器场景精心设计的任务指令；(3) 一套用于衡量任务完成度的执行式评估器。

<img src="/images/2508.14704v1/x2.jpg" alt="MCP-Universe 评估框架概览" style="width:90%; max-width:800px; margin:auto; display:block;">
<center>图2: MCP-Universe 评估框架概览。该框架根据任务规范动态配置LLM智能体、MCP服务器和执行式评估器。每次评估都涉及通过MCP协议介导的智能体-服务器交互，然后由自动化的执行式评估器进行客观评估，以确定任务是否成功完成。</center>

在形式上，一个任务 $\tau$ 被定义为元组 $(G, C, T\_{\mathrm{available}})$，其中 $G$ 是目标描述， $C$ 是初始上下文， $T\_{\mathrm{available}}$ 是可用工具集。智能体需要调用 $T\_{\mathrm{available}}$ 中的工具来完成目标 $G$。评估函数 $E$ 根据预定义的成功标准判定任务是否成功：


$$
E:M\times A\times\mathcal{T}\rightarrow\{0,1\}
$$


其中 $M$ 是语言模型集合, $A$ 是智能体架构集合, $\mathcal{T}$ 是任务集合。

### 评估框架设计
本文设计的评估框架能无缝协调各个组件，实现自动化、可复现的评测。
*   **LLM 管理器 (LLM Manager)**: 支持包括 GPT-5 和 Claude-4.0-Sonnet 在内的多个前沿 LLM，负责模型配置、API 管理和标准化提示格式。
*   **智能体构建器 (Agent Builder)**: 支持构建多种智能体，如 ReAct，并为其配置与 MCP 服务器通信的推理策略。
*   **MCP 服务器集成**: 框架与代表真实世界服务的多个 MCP 服务器无缝集成，确保评测任务反映真实环境而非模拟。
*   **执行式评估**: 采用自动化的执行式评估方法，通过领域特定的验证策略（如检查 GitHub 分支、验证谷歌地图站点类型）客观判断任务是否成功，避免了 LLM 裁判的主观性和成本。

### 真实的MCP服务器
MCP-Universe 的核心特点是依赖真实的 MCP 服务器，而非模拟环境，以确保评测的真实性。基准覆盖了 6 个核心领域和 11 个独立的 MCP 服务器。

<img src="/images/2508.14704v1/x3.jpg" alt="MCP-Universe 中的任务分布" style="width:80%; max-width:400px; margin:auto; display:block;">
<center>图3: MCP-Universe 中任务在不同应用领域的分布。</center>

*   **位置导航**: 使用官方 **Google Maps** MCP 服务器，进行路线规划、地点搜索等地理空间任务。
*   **仓库管理**: 使用 **GitHub** MCP 服务器，进行代码搜索、Issue 跟踪等版本控制操作。
*   **金融分析**: 使用 **Yahoo Finance** MCP 服务器，处理实时股价、股东信息等金融数据。
*   **3D 设计**: 使用 **Blender** MCP 服务器，进行 3D 建模、材质设置等计算机辅助设计任务。
*   **浏览器自动化**: 使用 **Playwright** MCP 服务器，执行网页导航、按钮点击等网页自动化操作。
*   **网页搜索**: 使用 **Google Search** 和 **Fetch** MCP 服务器，进行开放域信息检索。

此外，还集成了 **Notion**、**Weather**、**Date** 和 **Calculator** 等辅助服务器以增加任务复杂度。

### 任务与评估器
由于 MCP 是一个新概念，高质量的用例稀缺，本文手动设计了具有挑战性的任务，以反映真实应用场景。每个领域的任务都覆盖了最常见的使用场景。所有任务和评估器都经过多位作者的交叉审核，以确保其可行性、明确性和正确性。

评估器被设计为执行式的，以克服“LLM即裁判”范式在处理实时数据和潜在偏见方面的局限性。评估器分为三类：
1.  **格式评估器 (Format Evaluators)**: 检查智能体输出是否符合格式规范。
2.  **静态评估器 (Static Evaluators)**: 验证答案固定的任务，如历史事实或数据。
3.  **动态评估器 (Dynamic Evaluators)**: 验证答案随时间变化的任务，通过自动脚本获取实时基准真相。

如下表所示，基准共包含231个任务、11个MCP服务器、133个工具和84个独特的评估器，其中动态评估器占比最高（57.1%），突显了基准对实时能力的侧重。

<br>
<center><b>表2: MCP-Universe 中的关键统计数据</b></center>

| 统计项 | 数量 |
| --- | --- |
| 总任务数 | 231 (100%) |
| - 位置导航 | 45 (19.5%) |
| - 网页搜索 | 55 (23.8%) |
| - 浏览器自动化 | 39 (16.9%) |
| - 3D 设计 | 19 (8.2%) |
| - 金融分析 | 40 (17.3%) |
| - 仓库管理 | 33 (14.3%) |
| 总 MCP 服务器数量 | 11 |
| 服务器中的总工具数 | 133 |
| 总独立评估器数量 | 84 (100%) |
| - 格式评估器 | 4 (4.8%) |
| - 静态评估器 | 32 (38.1%) |
| - 动态评估器 | 48 (57.1%) |

# 实验结论

### SOTA模型性能
实验评估了多个前沿的闭源和开源 LLM。结果显示，即使是表现最好的模型，在 MCP-Universe 上也面临巨大挑战。

*   **总体性能**: GPT-5 以 43.72% 的总成功率（SR）位居第一，显著领先于 Grok-4（33.33%）和 Claude-4.0-Sonnet（29.44%）。这表明 SOTA 模型在真实的 MCP 环境中仍有很大的提升空间。
*   **领域差异**: 模型在不同领域的表现差异巨大。例如，GPT-5 在金融分析（67.50%）和 3D 设计（52.63%）领域表现出色，但在位置导航和仓库管理领域，所有模型的成功率都非常低，凸显了这些领域的特殊挑战。
*   **开源模型**: 最好的开源模型 GLM-4.5 (24.68%) 表现优于部分闭源模型，但与顶尖闭源模型之间仍存在显著差距。
*   **效率**: 在成功完成任务的平均步数（AS）上，o3 模型效率最高（4.82步），而性能更强的 GPT-5（8.22步）和 Grok-4（7.75步）则需要更多交互。

<br>
<center><b>表3: 在 MCP-Universe 基准上的性能对比</b></center>

| 模型 | 位置导航 | 仓库管理 | 金融分析 | 3D 设计 | 浏览器自动化 | 网页搜索 | 总体 | | |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| | SR | AE | AS |
| **闭源模型** | | | | | | | | | |
| GPT-5 | 33.33 | 30.30 | 67.50 | 52.63 | 35.90 | 45.45 | **43.72** | 60.23 | 8.22 |
| Grok-4 | 28.89 | 12.12 | 40.00 | 26.32 | 41.03 | 41.82 | **33.33** | 49.01 | 7.75 |
| Claude-4.0-Sonnet | 22.22 | 12.12 | 55.00 | 26.32 | 38.46 | 21.82 | **29.44** | 50.61 | 7.46 |
| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |
| **开源模型** | | | | | | | | | |
| GLM-4.5 | 17.78 | 9.09 | 50.00 | 26.32 | 15.38 | 27.27 | **24.68** | 41.16 | 7.33 |
| Kimi-K2 | 11.11 | 9.09 | 47.50 | 15.79 | 15.38 | 14.55 | **19.05** | 35.10 | 6.07 |
| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |

*注：SR-成功率(%)，AE-平均评估器得分(%)，AS-平均步数*

### 失败原因分析
通过分析不同评估器的通过率，发现模型失败的主要原因在于**内容生成**而非格式遵循。

<br>
<center><b>表4: 不同类型评估器的成功率</b></center>

| 模型 | 格式评估器 | 静态评估器 | 动态评估器 |
| --- | --- | --- | --- |
| GPT-5 | 88.89 | 61.92 | 65.96 |
| Grok-4 | 88.03 | 49.04 | 52.98 |
| Claude-4.0-Sonnet | 98.29 | 61.92 | 54.74 |
| ... | ... | ... | ... |

大多数模型在格式评估器上能达到 80% 以上的成功率，但在需要正确内容的静态和动态评估器上，成功率骤降至 40%-60%。这证明了本基准能够有效评估模型的核心推理与执行能力。

### 长上下文挑战
在位置导航、浏览器自动化等领域，任务通常需要处理长篇的工具返回信息，对模型的长上下文处理能力构成严峻考验。实验表明，随着交互步数的增加，上下文长度（Token数量）急剧增长。
<img src="/images/2508.14704v1/x4.jpg" alt="上下文长度增长与摘要智能体效果" style="width:85%; max-width:600px; margin:auto; display:block;">
<center>图4: (左) 交互步数增加时平均上下文长度的增长。(右) 引入摘要智能体对部分领域性能的影响。</center>

引入一个摘要智能体来压缩上下文的初步实验效果好坏参半：在某些场景下能提升性能，但在另一些场景下却会损害性能。这说明简单的上下文压缩方法不足以解决问题，需要更先进的长上下文处理策略。

### 未知工具挑战
错误分析显示，LLM 常常因为不熟悉工具的正确用法（如参数约束）而导致失败。
<img src="/images/2508.14704v1/x7.jpg" alt="未知工具挑战示例与探索阶段效果" style="width:85%; max-width:600px; margin:auto; display:block;">
<center>图5: (左) 一个未知工具挑战的例子。(右) 引入探索阶段对部分领域性能的影响。</center>

为此，本文设计了一个“探索-利用”实验，让智能体在执行任务前先自由探索工具以学习其用法。结果同样好坏参半：在某些领域提升了性能，但在其他领域无效甚至有害。这表明，目前的智能体适应未知工具的能力仍然有限，需要更鲁棒的自适应策略。

### 工具干扰与智能体框架对比
*   **工具干扰**: 当为任务连接更多不相关的 MCP 服务器时（从任务相关服务器增加到 7 个通用服务器），所有模型的性能都出现了明显下降。这证明本基准可用于评估智能体在复杂工具环境下的鲁棒性。

<img src="/images/2508.14704v1/x8.jpg" alt="连接更多无关MCP服务器的影响" style="width:85%; max-width:600px; margin:auto; display:block;">
<center>图6: 连接更多无关MCP服务器对性能的影响。</center>

*   **智能体框架对比**: 实验比较了 ReAct 与企业级智能体（如 Cursor Agent、OpenAI Agent SDK）的性能。结果显示，没有一个框架是普适最优的。例如，基于 o3 模型的 OpenAI Agent SDK 表现优于简单的 ReAct 框架，但基于 Claude-4.0-Sonnet 的 ReAct 框架却优于 Cursor Agent。这强调了智能体框架设计与模型本身同等重要，且最佳性能依赖于二者的有效结合。

<br>
<center><b>表5: 企业级智能体框架在MCP-Universe上的性能对比</b></center>

| 智能体框架 (骨干模型) | 位置导航 | ... | 总成功率 |
| --- | --- | --- | --- |
| **Claude-4.0-Sonnet** | | | |
| ReAct | 22.22 | ... | 29.44 |
| Cursor Agent | 22.22 | ... | 26.41 |
| **OpenAI o3** | | | |
| ReAct | 26.67 | ... | 26.41 |
| OpenAI Agent SDK | 28.89 | ... | 31.60 |

### 最终结论
本文引入了 MCP-Universe，一个用于在真实 MCP 环境中严格评估 LLM 的综合基准。通过在真实数据和执行式评估器上的测试，本基准揭示了当前 LLM 在长上下文处理、未知工具使用和跨领域泛化方面的关键短板。即使是顶尖模型和企业级智能体也难以应对 MCP 驱动任务的复杂性。MCP-Universe 为推动 LLM 在真实世界应用中的鲁棒性和实用性研究提供了一个宝贵的测试平台和未来的发展方向。