---
layout: default
title: "Limits of trust in medical AI"
---

# Limits of trust in medical AI

- **ArXiv URL**: http://arxiv.org/abs/2503.16692v2

- **作者**: Joshua Hatherley

- **发布机构**: University of Copenhagen

---

# TL;DR
本文论证了医疗人工智能（AI）系统仅能被依赖（be relied upon），而不能被信任（be trusted），因为真正的信任需要善意、动机和道德责任等人类特有的能动性（agency）要素，而AI的广泛应用可能通过取代人类医生的认知权威，导致医疗实践中人际信任关系的缺失。

# 关键定义
本文的核心论证建立在对几个关键概念的区分和界定之上：

*   **信任 (Trust) vs. 依赖 (Reliance)**：这是本文最核心的区分。**依赖**仅仅是对某个对象将按预期方式行事的预测性期望。而**信任**则是一种更深层次的关系，它不仅包含依赖，还涉及对被信任方具有善意（good will）或将信任方的利益纳入考量（encapsulated interest）的信念。
*   **认知权威 (Epistemic Authority)**：指在特定知识领域内，某个个体的判断和意见被认为是可靠和具有决定性分量的地位。本文探讨的是，随着AI性能超越人类，其可能取代医生在临床决策中的认知权威。
*   **人类中心主义困境 (Anthropocentric Predicament)**：由Paul Humphreys提出，指先进科技（如AI）创造出了超越人类能力的、非人类的认知权威，使我们人类面临如何理解和评估这些超人类方法的难题。

# 相关工作
本文旨在解决医疗AI发展中一个深刻的伦理问题：AI对医患信任关系的影响。

当前，人工智能，特别是深度学习技术，在医学诊断、预后和治疗选择等多个领域取得了显著进展，其表现在某些方面已经能与人类临床医生相媲美。这一趋势引发了关于AI在未来医疗实践中角色的讨论，形成了两种主要观点：
1.  **替代主义 (Substitutionism)**：认为AI最终将因其卓越性能而完全取代医生。
2.  **扩展主义 (Extensionism)**：认为AI将作为工具，扩展和增强医生的能力，但无法替代医生，因为医疗需要共情等AI不具备的人类特质。

尽管存在分歧，但两种观点都承认AI将对医疗实践产生颠覆性影响，并可能在关键临床任务中取代（displace）人类医生的角色。如果AI在性能上超越医生，医生将有认知上的义务去遵从AI的判断。这不仅是为了降低人为错误和医疗浪费，也是实现AI价值的关键。

本文的核心问题是：当医生在决策中必须让位于AI，从而使患者的依赖对象从医生转向AI系统时，这种认知权威的转移会对医患之间的信任关系造成何种影响？

# 本文方法
本文通过哲学思辨和概念分析，论证了AI系统不能成为真正“信任”的对象，其创新之处在于清晰地将“信任”与“依赖”进行了剥离，并指出了信任关系中不可或缺的人类能动性要素。

### 信任不仅仅是依赖
本文首先通过两个对比场景阐明了信任与依赖的区别：
1.  **依赖场景**：一个小偷**依赖**一位富裕的房主按时出门，以便他实施盗窃。
2.  **信任场景**：一位长期病患**信任**他的主治医生能帮助他缓解痛苦。

在这两个场景中，小偷和病人都对目标的行为有可靠的预期，但我们直观地认为只有后者才构成“信任”。本文认为，这种区别源于信任关系中存在的、而依赖关系中所缺乏的两个关键要素：**正确的动机**和**规范性义务**。

### AI缺乏信任所需的“动机”
真正的信任关系包含对被信任方动机的信念。
*   根据Russell Hardin的理论，信任需要相信对方的利益**封装 (encapsulated)**了你的利益。也就是说，对方会因为你的利益也是他/她的利益而采取行动。AI没有独立的“利益”，因此无法封装患者的利益。
*   根据Annette Baier和Karen Jones的理论，信任是对被信任方**善意 (good will)** 的乐观态度。信任者相信，在需要时，被信任方会因为“你正指望他/她”这一想法而积极地采取行动。AI系统没有动机，更没有善意或任何情感驱动。

因此，由于缺乏任何形式的动机——无论是封装的利益还是善意——AI系统无法成为这种丰富人际信任关系的对象。

### AI无法承担信任所产生的“规范性义务”
信任与依赖的另一个区别在于，信任会产生**规范性期望 (normative expectations)** 和道德义务。当患者信任医生时，医生就承担了必须尽力履行其职责的道德责任。
*   AI系统不是道德责任的合适主体。如果一个AI程序出现诊断错误导致患者死亡，我们不会去“指责”这个AI。相反，责任会被归咎于其设计者、使用者（医生）、监管机构或医院。
*   换言之，信任一个人类医生会给该医生带来道德责任；而信任（或更准确地说，依赖）一个AI，却将道德责任转移到了AI系统之外的其他人身上。

综上所述，由于AI系统缺乏**能动性 (agency)**，它既无法提供信任所需的动机基础，也无法承担信任关系所固有的道德责任。因此，说“信任”一个AI，在哲学意义上是不准确的，这实际上是把“依赖其可靠性”与真正的“人际信任”相混淆。

# 实验结论
本文是一篇哲学论证性质的文章，没有进行经验性实验，其结论是基于严密的逻辑推导得出的。

*   **核心结论**：“可信赖的AI (Trustworthy AI)”这一概念在根本上是一种误用。AI系统可以是**可靠的 (reliable)**，但它们不是**可信赖的 (trustworthy)** 的合适对象。因为“信任”是一种需要能动性（agency）的、专属于行动者（如人类）之间的互惠关系，就像我们依赖太阳升起但并不“信任”太阳一样。

*   **实践影响**：随着AI在医疗任务中的表现超越人类， clinicians 将有义务将他们的认知权威让渡给AI。这将导致患者的依赖对象从人类医生转向AI系统。

*   **潜在风险**：这种转变会带来一个严重的后果——牺牲掉医患之间那种深刻的、具有内在价值的人际信任关系。当患者只能依赖一个没有情感、没有动机、不承担责任的机器时，医疗实践中可能会出现信任的**赤字 (deficit)**。

*   **最终建议**：本文呼吁，在推进医疗AI技术的同时，需要对其社会和伦理影响进行更深入、更审慎的思考。目标应该是在最大化利用AI技术带来的效率和准确性优势的同时，有意识地设计和保留那些对医疗至关重要的、宝贵的人类元素，尤其是“信任”。